@software{100askTeamLv_lib_100ask2024,
  title = {{{100askTeam}}/Lv\_lib\_100ask},
  date = {2024-05-23T05:38:32Z},
  origdate = {2022-04-14T03:50:31Z},
  url = {https://github.com/100askTeam/lv_lib_100ask},
  urldate = {2024-05-23},
  abstract = {lv\_lib\_100ask is a reference for various out of the box schemes based on lvgl library or an enhanced interface for various components of lvgl library.},
  organization = {100askTeam},
  keywords = {100ask,lvgl}
}

@article{abolhassaniMachineLearningApproach,
  title = {Machine {{Learning Approach For User Interface Testing}}},
  author = {Abolhassani, Hassan},
  abstract = {A system and method are disclosed that enable automated testing of a user interface. The testing system includes a machine learning (ML) algorithm to test a user interface. The ML approach includes collecting training samples, creating an ML model and using the model to test the user interface. Training samples are collected by providing the users with diff files with the differences highlighted. A user is provided with options to specify if the differences are acceptable or not. The user classification and other attributes are used to train the model. When a new diff is created it may be fed to the trained network which results in prediction of acceptance or rejection (for example as "ACCEPT DIFF" or "REJECT DIFF") as output of the network. The system eliminates false positives in an automated way and thus reduces time spent by human inspectors to test user interface changes. BACKGROUND User interface testing tool may be used to ensure that changes in an application do not adversely affect the look and feel of it. A broad category of testing for this purpose includes screen diffing. Screen diffing is very useful, but is highly sensitive to noise. For example, a pixel shift of a browser page which makes naive pixel to pixel comparison fails when it should not. False positives are bottlenecks to usefulness of the screen diffing approaches. DESCRIPTION A system and method are disclosed that enables automated testing of a user interface. The testing system includes a machine learning (ML) algorithm to test a user interface. The method and system includes collection of training samples, creating an ML model and using the model to test the user interface as shown in FIG. 1.},
  langid = {english},
  file = {C:\Users\rini\Zotero\storage\4ME7LE9J\Abolhassani - Machine Learning Approach For User Interface Testi.pdf}
}

@article{alegrothLongtermUseVisual2017,
  title = {On the Long-Term Use of Visual Gui Testing in Industrial Practice: A Case Study},
  shorttitle = {On the Long-Term Use of Visual Gui Testing in Industrial Practice},
  author = {Alégroth, Emil and Feldt, Robert},
  date = {2017-12},
  journaltitle = {Empirical Software Engineering},
  shortjournal = {Empir Software Eng},
  volume = {22},
  number = {6},
  pages = {2937--2971},
  issn = {1382-3256, 1573-7616},
  doi = {10.1007/s10664-016-9497-6},
  url = {http://link.springer.com/10.1007/s10664-016-9497-6},
  urldate = {2024-01-24},
  langid = {english},
  file = {C:\Users\rini\Zotero\storage\KDIIJADH\Alégroth and Feldt - 2017 - On the long-term use of visual gui testing in indu.pdf}
}

@inproceedings{alegrothTransitioningManualSystem2013,
  title = {Transitioning {{Manual System Test Suites}} to {{Automated Testing}}: {{An Industrial Case Study}}},
  shorttitle = {Transitioning {{Manual System Test Suites}} to {{Automated Testing}}},
  booktitle = {2013 {{IEEE Sixth International Conference}} on {{Software Testing}}, {{Verification}} and {{Validation}}},
  author = {Alegroth, Emil and Feldt, Robert and Olsson, Helena H.},
  date = {2013-03},
  pages = {56--65},
  publisher = {IEEE},
  location = {Luxembourg, Luxembourg},
  doi = {10.1109/ICST.2013.14},
  url = {http://ieeexplore.ieee.org/document/6569716/},
  urldate = {2024-01-24},
  abstract = {Visual GUI testing (VGT) is an emerging technique that provides software companies with the capability to automate previously time-consuming, tedious, and fault prone manual system and acceptance tests. Previous work on VGT has shown that the technique is industrially applicable, but has not addressed the real-world applicability of the technique when used by practitioners on industrial grade systems. This paper presents a case study performed during an industrial project with the goal to transition from manual to automated system testing using VGT. Results of the study show that the VGT transition was successful and that VGT could be applied in the industrial context when performed by practitioners but that there were several problems that first had to be solved, e.g. testing of a distributed system, tool volatility. These problems and solutions have been presented together with qualitative, and quantitative, data about the benefits of the technique compared to manual testing, e.g. greatly improved execution speed, feasible transition and maintenance costs, improved bug finding ability. The study thereby provides valuable, and previously missing, contributions about VGT to both practitioners and researchers.},
  eventtitle = {2013 {{IEEE Sixth International Conference}} on {{Software Testing}}, {{Verification}} and {{Validation}} ({{ICST}})},
  isbn = {978-0-7695-4968-2 978-1-4673-5961-0},
  langid = {english},
  file = {C:\Users\rini\Zotero\storage\EJMQGJWH\Alegroth et al. - 2013 - Transitioning Manual System Test Suites to Automat.pdf}
}

@article{alegrothVisualGUITesting2015,
  title = {Visual {{GUI}} Testing in Practice: Challenges, Problemsand Limitations},
  shorttitle = {Visual {{GUI}} Testing in Practice},
  author = {Alégroth, Emil and Feldt, Robert and Ryrholm, Lisa},
  date = {2015-06},
  journaltitle = {Empirical Software Engineering},
  shortjournal = {Empir Software Eng},
  volume = {20},
  number = {3},
  pages = {694--744},
  issn = {1382-3256, 1573-7616},
  doi = {10.1007/s10664-013-9293-5},
  url = {http://link.springer.com/10.1007/s10664-013-9293-5},
  urldate = {2024-01-24},
  langid = {english},
  file = {C:\Users\rini\Zotero\storage\N3MBH3I9\Alégroth et al. - 2015 - Visual GUI testing in practice challenges, proble.pdf}
}

@incollection{altinbasGUIElementDetection2022,
  title = {{{GUI Element Detection}} from {{Mobile UI Images Using YOLOv5}}},
  booktitle = {Mobile {{Web}} and {{Intelligent Information Systems}}},
  author = {Altinbas, Mehmet Dogan and Serif, Tacha},
  editor = {Awan, Irfan and Younas, Muhammad and Poniszewska-Marańda, Aneta},
  date = {2022},
  volume = {13475},
  pages = {32--45},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-031-14391-5_3},
  url = {https://link.springer.com/10.1007/978-3-031-14391-5_3},
  urldate = {2024-01-24},
  abstract = {In mobile application development, building a consistent user interface (UI) might be a costly and time-consuming process. This is especially the case if an organization has a separate team for each mobile platform such as iOS and Android. In this regard, the companies that choose the native mobile app development path end up going through do-overs as the UI work done on one platform needs to be repeated for other platforms too. One of the tedious parts of UI design tasks is creating a graphical user interface (GUI). There are numerous tools and prototypes in the literature that aim to create feasible GUI automation solutions to speed up this process and reduce the labor workload. However, as the technologies evolve and improve new versions of existing algorithms are created and offered. Accordingly, this study aims to employ the latest version of YOLO, which is YOLOv5, to create a custom object detection model that recognizes GUI elements in a given UI image. In order to benchmark the newly trained YOLOv5 GUI element detection model, existing work from the literature and their data set is considered and used for comparison purposes. Therefore, this study makes use of 450 UI samples of the VINS dataset for testing, a similar amount for validation and the rest for model training. Then the findings of this work are compared with another study that has used the SSD algorithm and VINS dataset to train, validate and test its model, which showed that proposed algorithm outperformed SSD’s mean average precision (mAP) by 15.69\%.},
  isbn = {978-3-031-14390-8 978-3-031-14391-5},
  langid = {english},
  file = {C:\Users\rini\Zotero\storage\83FNHZYU\Altinbas and Serif - 2022 - GUI Element Detection from Mobile UI Images Using .pdf}
}

@article{batniComprehensiveStudyAutomation2018,
  title = {A {{Comprehensive Study}} on {{Automation}} Using {{Robot Framework}}},
  author = {Batni, Neha S and Shetty, Jyoti},
  date = {2018},
  volume = {9},
  number = {7},
  abstract = {Recently, many organizations have shown improvement in the automation of applications when compared to manual testing as manual testing consumes a lot of human work, it is time consuming and may also affect the accuracy of testing. Automation has shown significant improvement in terms of availability, efficiency and quality of software products. Automation using Robot Framework has advantages such as it is plain, powerful and easy extended framework that uses the keyword driven development. It has table form of creating syntax which makes it easy and test cases are in a uniform way. The framework also has the powerto make re-use the keywords from existing keyword and makes sure it provides easy extensibility. It has simple library API for creating custom test libraries in Python or Java, it also has instruction interface and XML based output files that ease integration into existing build infrastructure, for example continuous integration systems. All these features make sure that Robot Framework can be used to quickly automate test cases. This paper is a study on Robot framework and its usage in automation testing.},
  langid = {english},
  file = {C:\Users\rini\Zotero\storage\XM2KJ8EE\Batni and Shetty - 2018 - A Comprehensive Study on Automation using Robot Fr.pdf}
}

@article{berniDEFINITIONUSEREXPERIENCE2021,
  title = {{{FROM THE DEFINITION OF USER EXPERIENCE TO A FRAMEWORK TO CLASSIFY ITS APPLICATIONS IN DESIGN}}},
  author = {Berni, Aurora and Borgianni, Yuri},
  date = {2021-08},
  journaltitle = {Proceedings of the Design Society},
  shortjournal = {Proc. Des. Soc.},
  volume = {1},
  pages = {1627--1636},
  issn = {2732-527X},
  doi = {10.1017/pds.2021.424},
  url = {https://www.cambridge.org/core/product/identifier/S2732527X21004247/type/journal_article},
  urldate = {2024-01-24},
  abstract = {The concept of User Experience (UX) dates back to the 1990s, but a shared definition of UX is not available. As design integrates UX, different interpretations thereof can complicate the possibility to build upon previous literature and develop the field autonomously. Indeed, by analysing the literature, UX emerges as a cauldron of related and closely linked concepts. However, it is possible to find recurring attributes that emerge from those definitions, which are ascribable to two foci: the fundamental elements of the interaction (user, system, context) and typologies of experience (ergonomic, cognitive, and emotional). Those are used to build a framework. We have preliminarily investigated how UX is dealt with in design by mapping a sample of UX-related experimental articles published in design journals. We classified UX case studies based on the framework to individuate the UXs that emerge most frequently and the most studied ones in the design field. The two-focus framework allows the mapping of experiments involving UX in design, without highlighting specific favorable combinations. However, comprehensive studies dealing with all elements and UX typologies have not been found.},
  langid = {english},
  file = {C:\Users\rini\Zotero\storage\PZK4HW4W\Berni and Borgianni - 2021 - FROM THE DEFINITION OF USER EXPERIENCE TO A FRAMEW.pdf}
}

@online{BriefSummaryYOLOv8,
  title = {Brief Summary of {{YOLOv8}} Model Structure · {{Issue}} \#189 · Ultralytics/Ultralytics},
  url = {https://github.com/ultralytics/ultralytics/issues/189},
  urldate = {2024-05-23},
  abstract = {Model structure of YOLOv8 detection models(P5) - yolov8n/s/m/l/x: Changes compared to YOLOv5: Replace the C3 module with the C2f module Replace the first 6x6 Conv with 3x3 Conv in the Backbone Dele...},
  langid = {english},
  organization = {GitHub},
  file = {C:\Users\rini\Zotero\storage\ABSJKMEW\189.html}
}

@article{cavsakCavsakGUIComponent,
  title = {Cavsak et al. - {{GUI Component Detection Using YOLO}} and {{Faster-RCNN}}.Pdf},
  author = {Cavsak, Sena Nur and Deliahmetoglu, Aysu and Ay, Berhan Turku and Tanberk, Senem},
  abstract = {The graphical user interface (GUI) is crucial for communicating with software users. The detection of GUI elements holds significant importance for various software test automation tasks. In this study, two different object detection models such as YOLOv8 and Faster R-CNN are used to address challenging GUI component detection problems in mobile applications. Two different datasets were used. The first dataset is the RICO data, which consists of 5 classes. The second dataset consists of 600 mobile application screenshots collected from the open-source. This dataset consists of 7 classes and has been labeled and trained on Roboflow. In addition, an automation system was developed to test the object detection models for errors after the version change in the mobile application. With this test automation, the locations of the errors in the mobile application in realworld scenarios were determined and reported. In this experiment, 2 different data were trained in 2 different models, and the data we labeled gave the best result with a mAP value of 0.81 in the YOLOv8 model.},
  langid = {english},
  file = {C:\Users\rini\Zotero\storage\RC6ZKDV8\Cavsak et al. - GUI Component Detection Using YOLO and Faster-RCNN.pdf}
}

@inproceedings{chengMobileApplicationGUI2021,
  title = {Mobile {{Application GUI Similarity Comparison Based}} on {{Perceptual Hash}} for {{Automated Robot Testing}}},
  booktitle = {2021 {{International Conference}} on {{Intelligent Computing}}, {{Automation}} and {{Applications}} ({{ICAA}})},
  author = {Cheng, Jing and Wang, Wei},
  date = {2021-06},
  pages = {245--251},
  publisher = {IEEE},
  location = {Nanjing, China},
  doi = {10.1109/ICAA53760.2021.00052},
  url = {https://ieeexplore.ieee.org/document/9653505/},
  urldate = {2024-01-24},
  abstract = {Due to the rapid growth of mobile applications, the requirements for software testing speed are getting higher and higher. Therefore, automated testing is gradually replacing inefficient manual testing methods. However, most of the existing mobile applications are not open source code, and testing open source mobile applications is time-consuming and inefficient. Therefore, we propose automated robot testing based on black box testing. Firstly, YOLOV3 algorithm is used to obtain the information of mobile application interface components. Then, the perceptual hashing algorithm is used to identify the isomorphic GUI of mobile application, and the nodes that are the isomorphic GUI after the jump are merged to improve the test efficiency by simplifying the model. The experimental results show that the method can identify the isomorphic GUI quickly and accurately, and can better simplify the interface model and improve the test efficiency.},
  eventtitle = {2021 {{International Conference}} on {{Intelligent Computing}}, {{Automation}} and {{Applications}} ({{ICAA}})},
  isbn = {978-1-66543-730-1},
  langid = {english},
  file = {C:\Users\rini\Zotero\storage\HH7LLTLB\Cheng and Wang - 2021 - Mobile Application GUI Similarity Comparison Based.pdf}
}

@inproceedings{chengResearchRecognitionMethod2021,
  title = {Research on {{Recognition Method}} of {{Interface Elements Based}} on {{Machine Learning}}},
  booktitle = {2021 {{International Conference}} on {{Intelligent Computing}}, {{Automation}} and {{Applications}} ({{ICAA}})},
  author = {Cheng, Jing and Li, Junxian and Chen, Jingyi},
  date = {2021-06},
  pages = {233--236},
  publisher = {IEEE},
  location = {Nanjing, China},
  doi = {10.1109/ICAA53760.2021.00049},
  url = {https://ieeexplore.ieee.org/document/9653546/},
  urldate = {2024-01-24},
  abstract = {With the development of mobile Internet, the number of mobile applications has increased dramatically. The core of the mobile application is the interface interaction, and robots can be used to operate mobile applications for GUI automation testing. The recognition of interface elements is the basis of GUI automation test for mobile applications. The GUI models built based on the recognition results of existing interface recognition methods are too simple and the positions of the interface elements identified are not accurate, which cannot meet the testing requirements of mobile application robots. Hence, we propose an improved interface recognition algorithm based on the YOLOv3 algorithm to solve these problems. The experiment results demonstrate that the algorithm can effectively realize the layout classification and region recognition of the interlace based on the recognition of interface elements.},
  eventtitle = {2021 {{International Conference}} on {{Intelligent Computing}}, {{Automation}} and {{Applications}} ({{ICAA}})},
  isbn = {978-1-66543-730-1},
  langid = {english},
  file = {C:\Users\rini\Zotero\storage\HZ4FMQ5R\Cheng et al. - 2021 - Research on Recognition Method of Interface Elemen.pdf}
}

@online{ClearMLAgentGoogle,
  title = {{{ClearML Agent}} on {{Google Colab}} | {{ClearML}}},
  url = {https://clear.ml/docs/latest/docs/guides/ide/google_colab/},
  urldate = {2024-05-23},
  abstract = {Google Colab is a common development environment for data scientists. It supports a convenient IDE as well as},
  langid = {english},
  file = {C:\Users\rini\Zotero\storage\XYQ7F79H\google_colab.html}
}

@online{ClearMLContinuousMachine,
  title = {{{ClearML}} | {{The Continuous Machine Learning Company}}},
  url = {https://clear.ml/},
  urldate = {2024-05-23},
  abstract = {Easily develop, integrate, ship, and improve machine learning models at any scale with ClearML. Start free ={$>$}},
  langid = {american},
  organization = {ClearML}
}

@inproceedings{dekaRicoMobileApp2017,
  title = {Rico: {{A Mobile App Dataset}} for {{Building Data-Driven Design Applications}}},
  shorttitle = {Rico},
  booktitle = {Proceedings of the 30th {{Annual ACM Symposium}} on {{User Interface Software}} and {{Technology}}},
  author = {Deka, Biplab and Huang, Zifeng and Franzen, Chad and Hibschman, Joshua and Afergan, Daniel and Li, Yang and Nichols, Jeffrey and Kumar, Ranjitha},
  date = {2017-10-20},
  pages = {845--854},
  publisher = {ACM},
  location = {Québec City QC Canada},
  doi = {10.1145/3126594.3126651},
  url = {https://dl.acm.org/doi/10.1145/3126594.3126651},
  urldate = {2024-01-25},
  abstract = {Data-driven models help mobile app designers understand best practices and trends, and can be used to make predictions about design performance and support the creation of adaptive UIs. This paper presents Rico, the largest repository of mobile app designs to date, created to support five classes of data-driven applications: design search, UI layout generation, UI code generation, user interaction modeling, and user perception prediction. To create Rico, we built a system that combines crowdsourcing and automation to scalably mine design and interaction data from Android apps at runtime. The Rico dataset contains design data from more than 9.7k Android apps spanning 27 categories. It exposes visual, textual, structural, and interactive design properties of more than 72k unique UI screens. To demonstrate the kinds of applications that Rico enables, we present results from training an autoencoder for UI layout similarity, which supports queryby-example search over UIs.},
  eventtitle = {{{UIST}} '17: {{The}} 30th {{Annual ACM Symposium}} on {{User Interface Software}} and {{Technology}}},
  isbn = {978-1-4503-4981-9},
  langid = {english},
  file = {C:\Users\rini\Zotero\storage\VXNP6KCQ\Deka et al. - 2017 - Rico A Mobile App Dataset for Building Data-Drive.pdf}
}

@inproceedings{diasOnTargetTestAutomation2022,
  title = {On-{{Target Test Automation}} for an {{Embedded Digital Signal Processing Device}}},
  booktitle = {2022 {{IEEE International Conference}} on {{Consumer Electronics}} ({{ICCE}})},
  author = {Dias, Christian Charles and Soares, Clarice Sofia H. and Da Silva, Larissa T. and De Carvalho, Cleuves C. and Matos, Joao Paulo M. and Silva, Yago Luiz M. and Albuquerque, Danyllo W. and Santos, Danilo F. S.},
  date = {2022-01-07},
  pages = {1--2},
  publisher = {IEEE},
  location = {Las Vegas, NV, USA},
  doi = {10.1109/ICCE53296.2022.9730238},
  url = {https://ieeexplore.ieee.org/document/9730238/},
  urldate = {2024-01-24},
  abstract = {Test Automation of embedded systems is a challenging task, especially when dealing with specific components, such as Digital Signal Processing devices. In this article, we present an architecture that uses a well-adopted and generic automation framework, the Robot Framework, by adding an abstraction layer in the embedded device firmware to perform calls to DSP functions or firmware modules. The presented architecture enables the execution of tests on the target device integrated with other software components which are tested in the Robot Framework.},
  eventtitle = {2022 {{IEEE International Conference}} on {{Consumer Electronics}} ({{ICCE}})},
  isbn = {978-1-66544-154-4},
  langid = {english},
  file = {C:\Users\rini\Zotero\storage\SF9P3C3Y\Dias et al. - 2022 - On-Target Test Automation for an Embedded Digital .pdf}
}

@online{DINVDE08341,
  title = {{{DIN VDE}} 0834-1 {{VDE}} 0834-1:2016-06 - {{Normen}} - {{VDE VERLAG}}},
  url = {https://www.vde-verlag.de/normen/0800319/din-vde-0834-1-vde-0834-1-2016-06.html},
  urldate = {2024-01-30},
  file = {C:\Users\rini\Zotero\storage\M9FLPC4Q\din-vde-0834-1-vde-0834-1-2016-06.html}
}

@online{FlatFlatJpeg,
  title = {Flat/Flat/Jpeg.Py at Master · Xxyxyz/Flat},
  url = {https://github.com/xxyxyz/flat/blob/master/flat/jpeg.py},
  urldate = {2024-05-23},
  abstract = {Generative infrastructure for Python. Contribute to xxyxyz/flat development by creating an account on GitHub.},
  langid = {english},
  organization = {GitHub}
}

@inproceedings{gamalOwlEyeAIDriven2023,
  title = {Owl {{Eye}}: {{An AI-Driven Visual Testing Tool}}},
  shorttitle = {Owl {{Eye}}},
  booktitle = {2023 5th {{Novel Intelligent}} and {{Leading Emerging Sciences Conference}} ({{NILES}})},
  author = {Gamal, Ahmed and Emad, Reem and Mohamed, Taher and Mohamed, Omar and Hamdy, Ahmed and Ali, Sarah},
  date = {2023-10-21},
  pages = {312--315},
  publisher = {IEEE},
  location = {Giza, Egypt},
  doi = {10.1109/NILES59815.2023.10296575},
  url = {https://ieeexplore.ieee.org/document/10296575/},
  urldate = {2024-01-25},
  abstract = {Visual testing is a software testing technique that checks the visual aspects of a Graphical User Interface (GUI). It helps identify visual defects that other GUI testing techniques, such as functional and performance tests, may not detect. While functional testing verifies the correct behavior of applications, it is not effective in catching visual issues. This paper introduces an AI-driven visual tester, a novel approach to visual testing that enhances the detection of visual defects in application interfaces. The tester can effectively test any platform, using screenshots as input. The process begins with object detection in the screenshots, employing two methods: traditional image processing and deep learning. The detected objects are then transformed into a tree data structure for efficient analysis. Object matching is utilized to correlate objects between the reference and updated images. Finally, change analysis and classification are performed to identify and categorize changes in each object, such as translation, scaling, color change, or object removal.},
  eventtitle = {2023 5th {{Novel Intelligent}} and {{Leading Emerging Sciences Conference}} ({{NILES}})},
  isbn = {9798350381030},
  langid = {english},
  file = {C:\Users\rini\Zotero\storage\SNLQM5EU\Gamal et al. - 2023 - Owl Eye An AI-Driven Visual Testing Tool.pdf}
}

@article{garousiTestingEmbeddedSoftware2018,
  title = {Testing Embedded Software: {{A}} Survey of the Literature},
  shorttitle = {Testing Embedded Software},
  author = {Garousi, Vahid and Felderer, Michael and Karapıçak, Çağrı Murat and Yılmaz, Uğur},
  date = {2018-12},
  journaltitle = {Information and Software Technology},
  shortjournal = {Information and Software Technology},
  volume = {104},
  pages = {14--45},
  issn = {09505849},
  doi = {10.1016/j.infsof.2018.06.016},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0950584918301265},
  urldate = {2024-01-24},
  abstract = {Objective: However, reviewing and getting an overview of the entire state‐of‐the‐art and the –practice in this area is challenging for a practitioner or a (new) researcher. Also unfortunately, as a result, we often see that many companies reinvent the wheel (by designing a test approach new to them, but existing in the domain) due to not having an adequate overview of what already exists in this area. Method: To address the above need, we conducted and report in this paper a systematic literature review (SLR) in the form of a systematic literature mapping (SLM) in this area. After compiling an initial pool of 588 papers, a systematic voting about inclusion/exclusion of the papers was conducted among the authors, and our final pool included 312 technical papers. Results: Among the various aspects that we aim at covering, our review covers the types of testing topics studied, types of testing activity, types of test artifacts generated (e.g., test inputs or test code), and the types of industries in which studies have focused on, e.g., automotive and home appliances. Furthermore, we assess the benefits of this review by asking several active test engineers in the Turkish embedded software industry to review its findings and provide feedbacks as to how this review has benefitted them. Conclusion: The results of this review paper have already benefitted several of our industry partners in choosing the right test techniques / approaches for their embedded software testing challenges. We believe that it will also be useful for the large world‐wide community of software engineers and testers in the embedded software industry, by serving as an “index” to the vast body of knowledge in this important area. Our results will also benefit researchers in observing the latest trends in this area and for identifying the topics which need further investigations.},
  langid = {english},
  file = {C:\Users\rini\Zotero\storage\LWF4GD5X\Garousi et al. - 2018 - Testing embedded software A survey of the literat.pdf}
}

@inproceedings{givensExploringInternalState2013,
  title = {Exploring the Internal State of User Interfaces by Combining Computer Vision Techniques with Grammatical Inference},
  booktitle = {2013 35th {{International Conference}} on {{Software Engineering}} ({{ICSE}})},
  author = {Givens, Paul and Chakarov, Aleksandar and Sankaranarayanan, Sriram and Yeh, Tom},
  date = {2013-05},
  pages = {1165--1168},
  publisher = {IEEE},
  location = {San Francisco, CA, USA},
  doi = {10.1109/ICSE.2013.6606669},
  url = {http://ieeexplore.ieee.org/document/6606669/},
  urldate = {2024-01-25},
  abstract = {In this paper, we present a promising approach to systematically testing graphical user interfaces (GUI) in a platform independent manner. Our framework uses standard computer vision techniques through a python-based scripting language (Sikuli script) to identify key graphical elements in the screen and automatically interact with these elements by simulating keypresses and pointer clicks. The sequence of inputs and outputs resulting from the interaction is analyzed using grammatical inference techniques that can infer the likely internal states and transitions of the GUI based on the observations. Our framework handles a wide variety of user interfaces ranging from traditional pull down menus to interfaces built for mobile platforms such as Android and iOS. Furthermore, the automaton inferred by our approach can be used to check for potentially harmful patterns in the interface’s internal state machine such as design inconsistencies (eg,. a keypress does not have the intended effect) and mode confusion that can make the interface hard to use. We describe an implementation of the framework and demonstrate its working on a variety of interfaces including the user-interface of a safety critical insulin infusion pump that is commonly used by type-1 diabetic patients.},
  eventtitle = {2013 35th {{International Conference}} on {{Software Engineering}} ({{ICSE}})},
  isbn = {978-1-4673-3076-3 978-1-4673-3073-2},
  langid = {english},
  file = {C:\Users\rini\Zotero\storage\R9MJR7RA\Givens et al. - 2013 - Exploring the internal state of user interfaces by.pdf}
}

@online{HackXItUi_randomizer14d441c38a37850f4809471124bea3c36b9bc0b7,
  title = {{{HackXIt}}/Ui\_randomizer at 14d441c38a37850f4809471124bea3c36b9bc0b7},
  url = {https://github.com/HackXIt/ui_randomizer},
  urldate = {2024-05-23},
  abstract = {A project to generate random user interface using the LVGL framework for image data generation - HackXIt/ui\_randomizer at 14d441c38a37850f4809471124bea3c36b9bc0b7},
  langid = {english},
  organization = {GitHub},
  file = {C:\Users\rini\Zotero\storage\B64V3C5J\14d441c38a37850f4809471124bea3c36b9bc0b7.html}
}

@online{HackXItUidetector74795557012d5750fa35f1ff5a774d3172c6e953,
  title = {{{HackXIt}}/Ui-Detector at 74795557012d5750fa35f1ff5a774d3172c6e953},
  url = {https://github.com/HackXIt/ui-detector},
  urldate = {2024-05-23},
  abstract = {Source code for LVGL UI detector model with ClearML tasks - HackXIt/ui-detector at 74795557012d5750fa35f1ff5a774d3172c6e953},
  langid = {english},
  organization = {GitHub},
  file = {C:\Users\rini\Zotero\storage\FLNTZG6I\74795557012d5750fa35f1ff5a774d3172c6e953.html}
}

@inproceedings{holmbergIndustrialCaseStudyGUI2022,
  title = {An {{Industrial Case-Study}} on {{GUI Testing With RPA}}},
  booktitle = {2022 {{IEEE International Conference}} on {{Software Testing}}, {{Verification}} and {{Validation Workshops}} ({{ICSTW}})},
  author = {Holmberg, Mats and Dobslaw, Felix},
  date = {2022-04},
  pages = {199--206},
  publisher = {IEEE},
  location = {Valencia, Spain},
  doi = {10.1109/ICSTW55395.2022.00043},
  url = {https://ieeexplore.ieee.org/document/9787954/},
  urldate = {2024-01-24},
  abstract = {Automating regression tests can bring many benefits, such as increased testing frequency and improved bug-finding capabilities, leading to an overall positive impact on software quality. However, transitioning manual testing into automated testing is not possible without effort. In this work, we attempted to transition a manual test suite into an automated graphical user interface-based regression test suite using a Robotic Process Automation framework. The study reports on the efforts needed to implement the test suite and test case maintenance efforts. Furthermore, the study reports on bug-finding capabilities and discusses the feasibility of applying Robotic Process Automation for automated graphical user interface-based regression testing more broadly. Due to challenges related to current testing practices within the organization, only a small subset of the manual test cases could be successfully transitioned. The results indicate that the implementation and maintenance efforts patterns are similar to those of previous studies from the literature - even similar benefits and challenges could be observed. These findings suggest that Robotic Process Automation is feasible for graphical user interface-based regression testing, but further (long-term) investigations are needed in this area.},
  eventtitle = {2022 {{IEEE International Conference}} on {{Software Testing}}, {{Verification}} and {{Validation Workshops}} ({{ICSTW}})},
  isbn = {978-1-66549-628-5},
  langid = {english},
  file = {C:\Users\rini\Zotero\storage\AC76G7SK\Holmberg and Dobslaw - 2022 - An Industrial Case-Study on GUI Testing With RPA.pdf}
}

@online{HowCanStore2024,
  title = {How Can {{I}} Store a {{JPG}} Using Micropython and {{LVGL}} Snapshot? - {{Micropython}}},
  shorttitle = {How Can {{I}} Store a {{JPG}} Using Micropython and {{LVGL}} Snapshot?},
  date = {2024-03-17T15:44:42+00:00},
  url = {https://forum.lvgl.io/t/how-can-i-store-a-jpg-using-micropython-and-lvgl-snapshot/15135},
  urldate = {2024-05-23},
  abstract = {Greetings,  I have written some code in C for LVGL before, and there I used a library from 100askTeam (GitHub - 100askTeam/lv\_lib\_100ask: lv\_lib\_100ask is a reference for various out of the box schemes based on lvgl library or an enhanced interface for various components of lvgl library.) to take a screenshot and save it as JPG.  That library uses the tiny\_jpeg library to convert the raw image data into JPG format.  Recently I started porting my code to micropython to make development easier.  A...},
  langid = {english},
  organization = {LVGL Forum},
  file = {C:\Users\rini\Zotero\storage\2UL3N2RT\15135.html}
}

@online{HyperparameterOptimizationClearML,
  title = {Hyperparameter {{Optimization}} | {{ClearML}}},
  url = {https://clear.ml/docs/latest/docs/fundamentals/hpo},
  urldate = {2024-05-23},
  abstract = {What is Hyperparameter Optimization?},
  langid = {english},
  file = {C:\Users\rini\Zotero\storage\I5UHQXTH\hpo.html}
}

@online{IrfanViewOfficialHomepage,
  title = {{{IrfanView}} - {{Official Homepage}} - {{One}} of the {{Most Popular Viewers Worldwide}}},
  url = {https://www.irfanview.com/},
  urldate = {2024-01-30},
  file = {C:\Users\rini\Zotero\storage\9C5HRNJX\www.irfanview.com.html}
}

@software{jocherUltralyticsYOLO2023,
  title = {Ultralytics {{YOLO}}},
  author = {Jocher, Glenn and Chaurasia, Ayush and Qiu, Jing},
  date = {2023-01},
  origdate = {2022-09-11T16:39:45Z},
  url = {https://github.com/ultralytics/ultralytics},
  urldate = {2024-01-25},
  abstract = {NEW - YOLOv8 🚀 in PyTorch {$>$} ONNX {$>$} OpenVINO {$>$} CoreML {$>$} TFLite},
  version = {8.0.0}
}

@software{LibjpegturboLibjpegturbo2024,
  title = {Libjpeg-Turbo/Libjpeg-Turbo},
  date = {2024-05-23T21:10:27Z},
  origdate = {2015-07-27T07:11:54Z},
  url = {https://github.com/libjpeg-turbo/libjpeg-turbo},
  urldate = {2024-05-23},
  abstract = {Main libjpeg-turbo repository},
  organization = {libjpeg-turbo}
}

@article{linImprovingAccuracyAutomated2014,
  title = {Improving the {{Accuracy}} of {{Automated GUI Testing}} for {{Embedded Systems}}},
  author = {Lin, Ying-Dar and Chu, Edward T.-H. and Yu, Shang-Che and Lai, Yuan-Cheng},
  date = {2014-01},
  journaltitle = {IEEE Software},
  shortjournal = {IEEE Softw.},
  volume = {31},
  number = {1},
  pages = {39--45},
  issn = {0740-7459},
  doi = {10.1109/MS.2013.100},
  url = {http://ieeexplore.ieee.org/document/6576113/},
  urldate = {2024-01-25},
  langid = {english},
  file = {C:\Users\rini\Zotero\storage\LFI4ZFM4\Lin et al. - 2014 - Improving the Accuracy of Automated GUI Testing fo.pdf}
}

@online{liWidgetCaptioningGenerating2020,
  title = {Widget {{Captioning}}: {{Generating Natural Language Description}} for {{Mobile User Interface Elements}}},
  shorttitle = {Widget {{Captioning}}},
  author = {Li, Yang and Li, Gang and He, Luheng and Zheng, Jingjie and Li, Hong and Guan, Zhiwei},
  date = {2020-10-08},
  eprint = {2010.04295},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2010.04295},
  urldate = {2024-01-24},
  abstract = {Natural language descriptions of user interface (UI) elements such as alternative text are crucial for accessibility and language-based interaction in general. Yet, these descriptions are constantly missing in mobile UIs. We propose widget captioning, a novel task for automatically generating language descriptions for UI elements from multimodal input including both the image and the structural representations of user interfaces. We collected a largescale dataset for widget captioning with crowdsourcing. Our dataset contains 162,859 language phrases created by human workers for annotating 61,285 UI elements across 21,750 unique UI screens. We thoroughly analyze the dataset, and train and evaluate a set of deep model configurations to investigate how each feature modality as well as the choice of learning strategies impact the quality of predicted captions. The task formulation and the dataset as well as our benchmark models contribute a solid basis for this novel multimodal captioning task that connects language and user interfaces.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Human-Computer Interaction,Computer Science - Machine Learning},
  file = {C:\Users\rini\Zotero\storage\UHF9I3FM\Li et al. - 2020 - Widget Captioning Generating Natural Language Des.pdf}
}

@online{llcLiveDemosTest,
  title = {Live Demos - {{Test LVGL}} in Your Browser},
  author = {family=LLC, given=LVGL, given-i=LVGL},
  url = {https://lvgl.io/demos},
  urldate = {2024-05-24},
  abstract = {Check the available widgets, animations and effects right in your browser. You can easily try them on your MCU too!},
  langid = {english},
  organization = {LVGL},
  file = {C:\Users\rini\Zotero\storage\BNWKWWPG\demos.html}
}

@online{Lv_style_genLVGLDocumentation,
  title = {Lv\_style\_gen.h — {{LVGL}} Documentation},
  url = {https://docs.lvgl.io/9.1/API/misc/lv_style_gen.html#},
  urldate = {2024-05-23},
  file = {C:\Users\rini\Zotero\storage\AZITB4A7\lv_style_gen.html}
}

@online{LVGLLightVersatile,
  title = {{{LVGL}} - {{Light}} and {{Versatile Embedded Graphics Library}}},
  url = {https://lvgl.io/},
  urldate = {2024-01-30},
  file = {C:\Users\rini\Zotero\storage\SRWMCAVC\lvgl.io.html}
}

@online{LvglLv_binding_micropythonLVGL,
  title = {Lvgl/Lv\_binding\_micropython: {{LVGL}} Binding for {{MicroPython}}},
  url = {https://github.com/lvgl/lv_binding_micropython/tree/master},
  urldate = {2024-05-23},
  file = {C:\Users\rini\Zotero\storage\VZACMBHF\master.html}
}

@online{LvglLv_micropythonMicropython,
  title = {Lvgl/Lv\_micropython: {{Micropython}} Bindings to {{LVGL}} for {{Embedded}} Devices, {{Unix}} and {{JavaScript}}},
  url = {https://github.com/lvgl/lv_micropython},
  urldate = {2024-05-23}
}

@software{LvglLv_port_pc_vscode2024,
  title = {Lvgl/Lv\_port\_pc\_vscode},
  date = {2024-05-20T18:56:27Z},
  origdate = {2020-11-13T10:07:01Z},
  url = {https://github.com/lvgl/lv_port_pc_vscode},
  urldate = {2024-05-23},
  organization = {LVGL}
}

@report{nanniComparisonDifferentConvolutional2021,
  type = {preprint},
  title = {Comparison of {{Different Convolutional Neural Network Activation Functions}} and {{Methods}} for {{Building Ensembles}}},
  author = {Nanni, Loris and Brahnam, Sheryl and Paci, Michelangelo and Maguolo, Gianluca},
  date = {2021-03-05},
  institution = {MATHEMATICS \& COMPUTER SCIENCE},
  doi = {10.20944/preprints202103.0180.v1},
  url = {https://www.preprints.org/manuscript/202103.0180/v1},
  urldate = {2024-01-24},
  abstract = {Recently, much attention has been devoted to finding highly efficient and powerful activation functions for CNN layers. Because activation functions inject different nonlinearities between layers that affect performance, varying them is one method for building robust ensembles of CNNs. The objective of this study is to examine the performance of CNN ensembles made with different activation functions, including six new ones presented here: 2D Mexican ReLU, TanELU, MeLU+GaLU, Symmetric MeLU, Symmetric GaLU, and Flexible MeLU. The highest performing ensemble was built with CNNs having different activation layers that randomly replaced the standard ReLU. A comprehensive evaluation of the proposed approach was conducted across fifteen biomedical data sets representing various classification tasks. The proposed method was tested on two basic CNN architectures: Vgg16 and ResNet50. Results demonstrate the superiority in performance of this approach. The MATLAB source code for this study will be available at https://github.com/LorisNanni.},
  langid = {english},
  file = {C:\Users\rini\Zotero\storage\7NYZBWDY\Nanni et al. - 2021 - Comparison of Different Convolutional Neural Netwo.pdf}
}

@online{nasiriPowerLinearActivationFunctions2021,
  title = {{{PowerLinear Activation Functions}} with Application to the First Layer of {{CNNs}}},
  author = {Nasiri, Kamyar and Ghiasi-Shirazi, Kamaledin},
  date = {2021-08-20},
  eprint = {2108.09256},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2108.09256},
  urldate = {2024-01-24},
  abstract = {Convolutional neural networks (CNNs) have become the state-of-the-art tool for dealing with unsolved problems in computer vision and image processing. Since the convolution operator is a linear operator, several generalizations have been proposed to improve the performance of CNNs. One way to increase the capability of the convolution operator is by applying activation functions on the inner product operator. In this paper, we will introduce PowerLinear activation functions, which are based on the polynomial kernel generalization of the convolution operator. EvenPowLin functions are the main branch of the PowerLinear activation functions. This class of activation functions is saturated neither in the positive input region nor in the negative one. Also, the negative inputs are activated with the same magnitude as the positive inputs. These features made the EvenPowLin activation functions able to be utilized in the first layer of CNN architectures and learn complex features of input images. Additionally, EvenPowLin activation functions are used in CNN models to classify the inversion of grayscale images as accurately as the original grayscale images, which is significantly better than commonly used activation functions.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Neural and Evolutionary Computing},
  file = {C:\Users\rini\Zotero\storage\YNCRSIU4\Nasiri and Ghiasi-Shirazi - 2021 - PowerLinear Activation Functions with application .pdf}
}

@software{OpenaiOpenaipython2024,
  title = {Openai/Openai-Python},
  date = {2024-05-23T21:04:03Z},
  origdate = {2020-10-25T23:23:54Z},
  url = {https://github.com/openai/openai-python},
  urldate = {2024-05-23},
  abstract = {The official Python library for the OpenAI API},
  organization = {OpenAI},
  keywords = {openai,python}
}

@online{OptunaHyperparameterOptimization,
  title = {Optuna - {{A}} Hyperparameter Optimization Framework},
  url = {https://optuna.org/},
  urldate = {2024-05-23},
  abstract = {Optuna is an automatic hyperparameter optimization software framework, particularly designed for machine learning. It features an imperative, define-by-run style user API.},
  langid = {english},
  organization = {Optuna},
  file = {C:\Users\rini\Zotero\storage\VYNTKJFI\optuna.org.html}
}

@inproceedings{pelivaniComparativeStudyUI2022,
  title = {A {{Comparative Study}} of {{UI Testing Framework}}},
  booktitle = {2022 11th {{Mediterranean Conference}} on {{Embedded Computing}} ({{MECO}})},
  author = {Pelivani, Elis and Besimi, Adrian and Cico, Betim},
  date = {2022-06-07},
  pages = {1--5},
  publisher = {IEEE},
  location = {Budva, Montenegro},
  doi = {10.1109/MECO55406.2022.9797165},
  url = {https://ieeexplore.ieee.org/document/9797165/},
  urldate = {2024-01-24},
  abstract = {Software testing is gaining importance and most of the applications and delivery services have a threshold of test coverage and successful tests. There are different testing frameworks that can be used, but it is important that the companies choose based on their testing requirements, abilities, and capabilities. In this paper, we assess most of commonly popular software testing frameworks, test frames, and define their strength and weakness, Selenium IDE, Cypress, WebDriverIO and Robot Framework. For the research, there will be a real-life software with real test data. Test cases and test scenarios will be selected in order to have a better parameters measure and to have a realistic outcome for both the above points. We will do the evaluation and the comparison of automation testing tools and frameworks used to test the application.},
  eventtitle = {2022 11th {{Mediterranean Conference}} on {{Embedded Computing}} ({{MECO}})},
  isbn = {978-1-66546-828-2},
  langid = {english},
  file = {C:\Users\rini\Zotero\storage\YAT7ZLWU\Pelivani et al. - 2022 - A Comparative Study of UI Testing Framework.pdf}
}

@online{PipelineDecoratorClearML,
  title = {{{PipelineDecorator}} | {{ClearML}}},
  url = {https://clear.ml/docs/latest/docs/pipelines/pipelines_sdk_function_decorators},
  urldate = {2024-05-23},
  abstract = {Creating Pipelines Using Function Decorators},
  langid = {english},
  file = {C:\Users\rini\Zotero\storage\RGDY8IUV\pipelines_sdk_function_decorators.html}
}

@online{quachContinuousResourceManaged2019,
  title = {Continuous and {{Resource Managed Regression Testing}}: {{An Industrial Use Case}}},
  shorttitle = {Continuous and {{Resource Managed Regression Testing}}},
  author = {Quach, Tri and Oinonen, Tommi and Karjalainen, Antti},
  date = {2019-10-23},
  eprint = {1905.01928},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1905.01928},
  urldate = {2024-01-24},
  abstract = {Regression testing is an important part of quality control in both software and embedded products, where hardware is involved. It is also one of the most expensive and time consuming part of the product cycle. To improve the cost effectiveness of the development cycle and the regression testing, we use test case prioritisation and selection techniques to run more important test cases earlier in the testing process.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Software Engineering},
  file = {C:\Users\rini\Zotero\storage\SB48NQ4B\Quach et al. - 2019 - Continuous and Resource Managed Regression Testing.pdf}
}

@online{redmonYouOnlyLook2016,
  title = {You {{Only Look Once}}: {{Unified}}, {{Real-Time Object Detection}}},
  shorttitle = {You {{Only Look Once}}},
  author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
  date = {2016-05-09},
  eprint = {1506.02640},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1506.02640},
  urldate = {2024-01-24},
  abstract = {We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C:\Users\rini\Zotero\storage\RYFA8AM5\Redmon et al. - 2016 - You Only Look Once Unified, Real-Time Object Dete.pdf}
}

@online{ReleaseFinalPaper,
  title = {Release {{Final}} Paper Results · {{HackXIt}}/Ui-Detector-Paper},
  url = {https://github.com/HackXIt/ui-detector-paper/releases/tag/final-paper-results},
  urldate = {2024-05-24},
  abstract = {In this release, the data of the final thesis results is uploaded. It contains three datasets of the two generator modes: 04da75baa7084aee83f3b31602c408c2\_random 5c8e248bdcc24ee2bb57760f0961c690\_d...},
  langid = {english},
  organization = {GitHub},
  file = {C:\Users\rini\Zotero\storage\RLCPCPWV\final-paper-results.html}
}

@online{RobotFramework,
  title = {Robot {{Framework}}},
  url = {https://robotframework.org/},
  urldate = {2024-01-25},
  file = {C:\Users\rini\Zotero\storage\PY8GY6T6\robotframework.org.html}
}

@incollection{selcukComparisonYOLOv5YOLOv82023,
  title = {A {{Comparison}} of {{YOLOv5}} and {{YOLOv8}} in the {{Context}} of {{Mobile UI Detection}}},
  booktitle = {Mobile {{Web}} and {{Intelligent Information Systems}}},
  author = {Selcuk, Burcu and Serif, Tacha},
  editor = {Younas, Muhammad and Awan, Irfan and Grønli, Tor-Morten},
  date = {2023},
  volume = {13977},
  pages = {161--174},
  publisher = {Springer Nature Switzerland},
  location = {Cham},
  doi = {10.1007/978-3-031-39764-6_11},
  url = {https://link.springer.com/10.1007/978-3-031-39764-6_11},
  urldate = {2024-01-24},
  isbn = {978-3-031-39763-9 978-3-031-39764-6},
  langid = {english},
  file = {C:\Users\rini\Zotero\storage\V8XMDVB6\Selcuk and Serif - 2023 - A Comparison of YOLOv5 and YOLOv8 in the Context o.pdf}
}

@online{sharmaEvaluatingCNNOscillatory2022,
  title = {Evaluating {{CNN}} with {{Oscillatory Activation Function}}},
  author = {Sharma, Jeevanshi},
  date = {2022-11-13},
  eprint = {2211.06878},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2211.06878},
  urldate = {2024-01-24},
  abstract = {The reason behind CNN’s capability to learn high-dimensional complex features from the images is the non-linearity introduced by the activation function. Several advanced activation functions have been discovered to improve the training process of neural networks, as choosing an activation function is a crucial step in the modeling. Recent research has proposed using an oscillating activation function to solve classification problems inspired by the human brain cortex. [1] This paper explores the performance of one of the CNN architecture ALexNet on MNIST and CIFAR10 dataset using oscillatory activation function (GCU) and some other commonly used activation functions like ReLu, PReLu, Mish.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {C:\Users\rini\Zotero\storage\WD2Y3L8D\Sharma - 2022 - Evaluating CNN with Oscillatory Activation Functio.pdf}
}

@online{soutoTimeSpaceEfficientRegression2017,
  title = {Time-{{Space Efficient Regression Testing}} for {{Configurable Systems}}},
  author = {Souto, Sabrina and family=Amorim, given=Marcelo, prefix=d', useprefix=true},
  date = {2017-02-11},
  eprint = {1702.03457},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1702.03457},
  urldate = {2024-01-24},
  abstract = {Configurable systems are those that can be adapted from a set of options. They are prevalent and testing them is important and challenging. Existing approaches for testing configurable systems are either unsound (i.e., they can miss fault-revealing configurations) or do not scale.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Software Engineering,D.2.5},
  file = {C:\Users\rini\Zotero\storage\U4QS5ADE\Souto and d'Amorim - 2017 - Time-Space Efficient Regression Testing for Config.pdf}
}

@online{strandbergAutomatedSystemLevelSoftware2021,
  title = {Automated {{System-Level Software Testing}} of {{Industrial Networked Embedded Systems}}},
  author = {Strandberg, Per Erik},
  date = {2021-11-16},
  eprint = {2111.08312},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2111.08312},
  urldate = {2024-01-24},
  abstract = {Embedded systems are ubiquitous and play critical roles in management systems for industry and transport. Software failures in these domains may lead to loss of production or even loss of life, so the software in these systems needs to be reliable. Software testing is a standard approach for quality assurance of embedded software, and many software development processes strive for test automation. Out of the many challenges for successful software test automation, this thesis addresses five: (i) understanding how updated software reaches a test environment, how testing is conducted in the test environment, and how test results reach the developers that updated the software in the first place; (ii) selecting which test cases to execute in a test suite given constraints on available time and test systems; (iii) given that the test cases are run on different configurations of connected devices, selecting which hardware to use for each test case to be executed; (iv) analyzing test cases that, when executed over time on evolving software, testware or hardware revisions, appear to randomly fail; and (v) making test results information actionable with test results exploration and visualization.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Software Engineering},
  file = {C:\Users\rini\Zotero\storage\FL9X8V7T\Strandberg - 2021 - Automated System-Level Software Testing of Industr.pdf}
}

@online{strandbergWestermoTestResults2022,
  title = {The {{Westermo}} Test Results Data Set},
  author = {Strandberg, Per Erik},
  date = {2022-11-24},
  eprint = {2211.13622},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2211.13622},
  urldate = {2024-01-24},
  abstract = {There is a growing body of knowledge in the computer science, software engineering, software testing and software test automation disciplines. However, there is a challenge for researchers to evaluate their research findings, innovations and tools due to lack of realistic data. This paper presents the Westermo test results data set, more than one million verdicts from testing of embedded systems, from more than five hundred consecutive days of nightly testing. The data also contains information on code changes in both the software under test and the test framework used for testing. This data set can support the research community in particular with respect to the regression test selection problem, flaky tests, test results visualization, etc.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Software Engineering},
  file = {C:\Users\rini\Zotero\storage\MX7ZN6DH\Strandberg - 2022 - The Westermo test results data set.pdf}
}

@online{UidetectorSchemaDesign_file,
  title = {Ui-Detector/Schema/Design\_file.Schema.Json at 74795557012d5750fa35f1ff5a774d3172c6e953 · {{HackXIt}}/Ui-Detector},
  url = {https://github.com/HackXIt/ui-detector/blob/74795557012d5750fa35f1ff5a774d3172c6e953/schema/design_file.schema.json},
  urldate = {2024-05-23}
}

@online{UidetectorSrcGenerate,
  title = {Ui-Detector/Src/Generate.Py at 74795557012d5750fa35f1ff5a774d3172c6e953 · {{HackXIt}}/Ui-Detector ({{GPT}} Design System Prompt)},
  url = {https://github.com/HackXIt/ui-detector/blob/74795557012d5750fa35f1ff5a774d3172c6e953/src/generate.py#L612},
  urldate = {2024-05-23},
  file = {C:\Users\rini\Zotero\storage\Q4LEERJA\generate.html}
}

@article{ulewiczIndustriallyApplicableSystem2018,
  title = {Industrially {{Applicable System Regression Test Prioritization}} in {{Production Automation}}},
  author = {Ulewicz, Sebastian and Vogel-Heuser, Birgit},
  date = {2018-10},
  journaltitle = {IEEE Transactions on Automation Science and Engineering},
  shortjournal = {IEEE Trans. Automat. Sci. Eng.},
  volume = {15},
  number = {4},
  pages = {1839--1851},
  issn = {1545-5955, 1558-3783},
  doi = {10.1109/TASE.2018.2810280},
  url = {https://ieeexplore.ieee.org/document/8320514/},
  urldate = {2024-01-24},
  abstract = {When changes are performed on an automated production system (aPS), new faults can be accidentally introduced into the system, which are called regressions. A common method for finding these faults is regression testing. In most cases, this regression testing process is performed under high time pressure and on-site in a very uncomfortable environment. Until now, there has been no automated support for finding and prioritizing system test cases regarding the fully integrated aPS that are suitable for finding regressions. Thus, the testing technician has to rely on personal intuition and experience, possibly choosing an inappropriate order of test cases, finding regressions at a very late stage of the test run. Using a suitable prioritization, this iterative process of finding and fixing regressions can be streamlined and a lot of time can be saved by executing test cases likely to identify new regressions earlier. Thus, an approach is presented in this paper that uses previously acquired runtime data from past test executions and performs a change identification and impact analysis to prioritize test cases that have a high probability to unveil regressions caused by side effects of a system change. The approach was developed in cooperation with reputable industrial partners active in the field of aPS engineering, ensuring a development in line with industrial requirements. An industrial case study and an expert evaluation were performed, showing promising results.},
  langid = {english},
  file = {C:\Users\rini\Zotero\storage\Z4XBQXHE\Ulewicz and Vogel-Heuser - 2018 - Industrially Applicable System Regression Test Pri.pdf}
}

@online{ultralyticsClearML,
  title = {{{ClearML}}},
  author = {Ultralytics},
  url = {https://docs.ultralytics.com/integrations/clearml},
  urldate = {2024-05-23},
  abstract = {Learn how to streamline and optimize your YOLOv8 model training with ClearML. This guide provides insights into integrating ClearML's MLOps tools for efficient model training, from initial setup to advanced experiment tracking and model management.},
  langid = {english},
  file = {C:\Users\rini\Zotero\storage\G8ADDJS5\clearml.html}
}

@online{ultralyticsTuner,
  title = {Tuner},
  author = {Ultralytics},
  url = {https://docs.ultralytics.com/reference/engine/tuner},
  urldate = {2024-05-23},
  abstract = {Explore the Ultralytics Tuner, a powerful tool designed for hyperparameter tuning of YOLO models to optimize performance across various tasks like object detection, image classification, and more.},
  langid = {english},
  file = {C:\Users\rini\Zotero\storage\T9GHVF5N\tuner.html}
}

@online{UltralyticsUltralyticsV8,
  title = {Ultralytics/Ultralytics at v8.1.0},
  url = {https://github.com/ultralytics/ultralytics/tree/v8.1.0},
  urldate = {2024-05-23},
  file = {C:\Users\rini\Zotero\storage\AHAENYWB\v8.1.html}
}

@online{VisocallIPModerne,
  title = {Visocall IP | Moderne IP Kommunikationslösung},
  url = {https://www.schrack-seconet.com/de/healthcare/kommunikationssystem-visocall-ip/},
  urldate = {2024-01-30},
  abstract = {Wir schützen Leben. Wir sichern Werte. Schrack Seconet ist ein international führendes Hightech-Unternehmen aus Österreich.},
  langid = {ngerman},
  organization = {Schrack Seconet AG},
  file = {C:\Users\rini\Zotero\storage\2CMT2BKP\kommunikationssystem-visocall-ip.html}
}

@article{walkerSoftwareTestAutomation2020a,
  title = {Software {{Test Automation}} with {{Robot Framework}}},
  author = {Walker, Joshua T.},
  date = {2020-10-15},
  journaltitle = {International Journal of Computer Applications},
  shortjournal = {IJCA},
  volume = {175},
  number = {25},
  pages = {8--14},
  issn = {09758887},
  doi = {10.5120/ijca2020920784},
  url = {http://www.ijcaonline.org/archives/volume175/number25/walker-2020-ijca-920784.pdf},
  urldate = {2024-01-24},
  abstract = {Automated testing allows for releases to be quickly verified and regression tested multiple times over a product’s development period, providing a level of quality assurance unmatched by a purely manual process. This article discusses the driving objectives, implementation strategy, and general overview of a test automation framework created using the open source project Robot Framework as its test automation core. The framework described in this article has been used to provide test automation capabilities to both Graphical User Interface (GUI) based Windows applications and embedded systems. In addition to an overview of the developed framework, this article also provides the results of automated testing for multiple releases and a discussion of selected lessons learned from its creation and maintenance.},
  langid = {english},
  file = {C:\Users\rini\Zotero\storage\6EL2NV35\Walker - 2020 - Software Test Automation with Robot Framework.pdf}
}

@online{wangYOLOv9LearningWhat2024,
  title = {{{YOLOv9}}: {{Learning What You Want}} to {{Learn Using Programmable Gradient Information}}},
  shorttitle = {{{YOLOv9}}},
  author = {Wang, Chien-Yao and Yeh, I.-Hau and Liao, Hong-Yuan Mark},
  date = {2024-02-28},
  eprint = {2402.13616},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2402.13616},
  url = {http://arxiv.org/abs/2402.13616},
  urldate = {2024-05-23},
  abstract = {Today's deep learning methods focus on how to design the most appropriate objective functions so that the prediction results of the model can be closest to the ground truth. Meanwhile, an appropriate architecture that can facilitate acquisition of enough information for prediction has to be designed. Existing methods ignore a fact that when input data undergoes layer-by-layer feature extraction and spatial transformation, large amount of information will be lost. This paper will delve into the important issues of data loss when data is transmitted through deep networks, namely information bottleneck and reversible functions. We proposed the concept of programmable gradient information (PGI) to cope with the various changes required by deep networks to achieve multiple objectives. PGI can provide complete input information for the target task to calculate objective function, so that reliable gradient information can be obtained to update network weights. In addition, a new lightweight network architecture -- Generalized Efficient Layer Aggregation Network (GELAN), based on gradient path planning is designed. GELAN's architecture confirms that PGI has gained superior results on lightweight models. We verified the proposed GELAN and PGI on MS COCO dataset based object detection. The results show that GELAN only uses conventional convolution operators to achieve better parameter utilization than the state-of-the-art methods developed based on depth-wise convolution. PGI can be used for variety of models from lightweight to large. It can be used to obtain complete information, so that train-from-scratch models can achieve better results than state-of-the-art models pre-trained using large datasets, the comparison results are shown in Figure 1. The source codes are at: https://github.com/WongKinYiu/yolov9.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\rini\\Zotero\\storage\\VJQPX8GJ\\Wang et al. - 2024 - YOLOv9 Learning What You Want to Learn Using Prog.pdf;C\:\\Users\\rini\\Zotero\\storage\\LC9IYDQV\\2402.html}
}

@online{WidgetsLVGLDocumentation,
  title = {Widgets — {{LVGL}} Documentation},
  url = {https://docs.lvgl.io/master/widgets/index.html},
  urldate = {2024-05-24},
  file = {C:\Users\rini\Zotero\storage\DP7Y5VYT\index.html}
}

@inproceedings{yendriDevelopmentComponentRecognition2022,
  title = {Development of {{Component Recognition Applications}} and {{Labor Tools Based}} on {{Android}} and {{Tiny Yolo Network}} ({{Case Study}}: {{Signal}} and {{System Laboratory}})},
  shorttitle = {Development of {{Component Recognition Applications}} and {{Labor Tools Based}} on {{Android}} and {{Tiny Yolo Network}} ({{Case Study}}},
  booktitle = {2022 {{International Symposium}} on {{Information Technology}} and {{Digital Innovation}} ({{ISITDI}})},
  author = {Yendri, Dodon and Arief, Lathifah and Yolanda, Desta and {Humaira} and Muhammad, Fauzan},
  date = {2022-07-27},
  pages = {101--107},
  publisher = {IEEE},
  location = {Padang, Indonesia},
  doi = {10.1109/ISITDI55734.2022.9944397},
  url = {https://ieeexplore.ieee.org/document/9944397/},
  urldate = {2024-01-25},
  abstract = {Practicum activities in the laboratory usually equipped with tools and components that must be prepared in advance. This study aims to develop an application for recognizing laboratory tools and components. The application is designed for Android-baced devices by utilizing the smartphone camera and developed using Tiny YOLO. The development follows System Development Life Cycle (SDLC) methodology using waterfall model. The system then tested by training data on 1,666 image objects obtained from Google in the form of laboratory tools and components such as Arduino, Raspberry Pi, HC-05 sensor, Esp-32 Module, Multimeter, Oscilloscope, and Function Generator. The results showed that the system can detect components and laboratory tools at an optimal distance of 25-35 cm and the accuracy of object detection is influenced by the light conditions in the. From several components tested, the object detection accuracy rate for Arduino Uno is 73.33\%, Raspberry Pi is 82.5\%, Bluetooth HC-05 module is 86.84\%, Esp32 module is 84.37\%, Multimeter is 80.6\%, Oscilloscope is 76.31\% and 80\% function generator.},
  eventtitle = {2022 {{International Symposium}} on {{Information Technology}} and {{Digital Innovation}} ({{ISITDI}})},
  isbn = {978-1-66546-116-0},
  langid = {english},
  file = {C:\Users\rini\Zotero\storage\R4G85EM5\Yendri et al. - 2022 - Development of Component Recognition Applications .PDF}
}

@article{zhangDeepLearningBasedMobile2020,
  title = {Deep {{Learning-Based Mobile Application Isomorphic GUI Identification}} for {{Automated Robotic Testing}}},
  author = {Zhang, Tao and Liu, Ying and Gao, Jerry and Gao, Li Peng and Cheng, Jing},
  date = {2020-07},
  journaltitle = {IEEE Software},
  shortjournal = {IEEE Softw.},
  volume = {37},
  number = {4},
  pages = {67--74},
  issn = {0740-7459, 1937-4194},
  doi = {10.1109/MS.2020.2987044},
  url = {https://ieeexplore.ieee.org/document/9064552/},
  urldate = {2024-01-24},
  langid = {english},
  file = {C:\Users\rini\Zotero\storage\TMEZJD6D\Zhang et al. - 2020 - Deep Learning-Based Mobile Application Isomorphic .pdf}
}

@online{zhangEffectiveFastMemoryEfficient2018,
  title = {Effective, {{Fast}}, and {{Memory-Efficient Compressed Multi-function Convolutional Neural Networks}} for {{More Accurate Medical Image Classification}}},
  author = {Zhang, Luna M.},
  date = {2018-11-29},
  eprint = {1811.11996},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1811.11996},
  urldate = {2024-01-24},
  abstract = {Convolutional Neural Networks (CNNs) usually use the same activation function, such as RELU, for all convolutional layers. There are performance limitations of just using RELU. In order to achieve better classification performance, reduce training and testing times, and reduce power consumption and memory usage, a new "Compressed Multi-function CNN" is developed. Google’s Inception-V4, for example, is a very deep CNN that consists of 4 Inception-A blocks, 7 InceptionB blocks, and 3 Inception-C blocks. RELU is used for all convolutional layers. A new "Compressed Multi-function Inception-V4" (CM I) that can use different activation functions is created with k Inception-A blocks, m Inception-B blocks, and n Inception-C blocks where k ∈ \{1, 2, 3, 4\}, m ∈ \{1, 2, 3, 4, 5, 6, 7\}, n ∈ \{1, 2, 3\}, and (k + m + n) {$<$} 14. For performance analysis, a dataset for classifying brain MRI images into one of the four stages of Alzheimer’s disease is used to compare three CM I architectures with Inception-V4 in terms of F1-score, training and testing times (related to power consumption), and memory usage (model size). Overall, simulations show that the new CM I models can outperform both the commonly used Inception-V4 and Inception-V4 using different activation functions. In the future, other "Compressed Multi-function CNNs", such as "Compressed Multi-function ResNets and DenseNets" that have a reduced number of convolutional blocks using different activation functions, will be developed to further increase classification accuracy, reduce training and testing times, reduce computational power, and reduce memory usage (model size) for building more effective healthcare systems, such as implementing accurate and convenient disease diagnosis systems on mobile devices that have limited battery power and memory.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C:\Users\rini\Zotero\storage\PP45DBDD\Zhang - 2018 - Effective, Fast, and Memory-Efficient Compressed M.PDF}
}

@article{zhangMachineVisionbasedTesting2022,
  title = {Machine Vision-Based Testing Action Recognition Method for Robotic Testing of Mobile Application},
  author = {Zhang, Tao and Su, Zhengqi and Cheng, Jing and Xue, Feng and Liu, Shengyu},
  date = {2022-08},
  journaltitle = {International Journal of Distributed Sensor Networks},
  shortjournal = {International Journal of Distributed Sensor Networks},
  volume = {18},
  number = {8},
  pages = {155013292211153},
  issn = {1550-1329, 1550-1477},
  doi = {10.1177/15501329221115375},
  url = {http://journals.sagepub.com/doi/10.1177/15501329221115375},
  urldate = {2024-01-24},
  abstract = {The explosive growth and rapid version iteration of various mobile applications have brought enormous workloads to mobile application testing. Robotic testing methods can efficiently handle repetitive testing tasks, which can compensate for the accuracy of manual testing and improve the efficiency of testing work. Vision-based robotic testing identifies the types of test actions by analyzing expert test videos and generates expert imitation test cases. The mobile application expert imitation testing method uses machine learning algorithms to analyze the behavior of experts imitating test videos, generates test cases with high reliability and reusability, and drives robots to execute test cases. However, the difficulty of estimating multi-dimensional gestures in 2D images leads to complex algorithm steps, including tracking, detection, and recognition of dynamic gestures. Hence, this article focuses on the analysis and recognition of test actions in mobile application robot testing. Combined with the improved YOLOv5 algorithm and the ResNet-152 algorithm, a visual modeling method of mobile application test action based on machine vision is proposed. The precise localization of the hand is accomplished by injecting dynamic anchors, attention mechanism, and the weighted boxes fusion in the YOLOv5 algorithm. The improved algorithm recognition accuracy increased from 82.6\% to 94.8\%. By introducing the pyramid context awareness mechanism into the ResNet-152 algorithm, the accuracy of test action classification is improved. The accuracy of the test action classification was improved from 72.57\% to 76.84\%. Experiments show that this method can reduce the probability of multiple detections and missed detection of test actions, and improve the accuracy of test action recognition.},
  langid = {english},
  file = {C:\Users\rini\Zotero\storage\FY86KK5M\Zhang et al. - 2022 - Machine vision-based testing action recognition me.pdf}
}

@online{zhangMultifunctionConvolutionalNeural2018,
  title = {Multi-Function {{Convolutional Neural Networks}} for {{Improving Image Classification Performance}}},
  author = {Zhang, Luna M.},
  date = {2018-05-29},
  eprint = {1805.11788},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1805.11788},
  urldate = {2024-01-24},
  abstract = {Traditional Convolutional Neural Networks (CNNs) typically use the same activation function (usually ReLU) for all neurons with non-linear mapping operations. For example, the deep convolutional architecture Inception-v4 uses ReLU. To improve the classification performance of traditional CNNs, a new "Multi-function Convolutional Neural Network" (MCNN) is created by using different activation functions for different neurons. For n neurons and m different activation functions, there are a total of mn − m MCNNs and only m traditional CNNs. Therefore, the best model is very likely to be chosen from MCNNs because there are mn − 2m more MCNNs than traditional CNNs. For performance analysis, two different datasets for two applications (classifying handwritten digits from the MNIST database and classifying brain MRI images into one of the four stages of Alzheimer’s disease (AD)) are used. For both applications, an activation function is randomly selected for each layer of a MCNN. For the AD diagnosis application, MCNNs using a newly created multi-function Inception-v4 architecture are constructed. Overall, simulations show that MCNNs can outperform traditional CNNs in terms of multi-class classification accuracy for both applications. An important future research work will be to efficiently select the best MCNN from mn − m candidate MCNNs. Current CNN software only provides users with partial functionality of MCNNs since different layers can use different activation functions but not individual neurons in the same layer. Thus, modifying current CNN software systems such as ResNets, DenseNets, and Dual Path Networks by using multiple activation functions and developing more effective and faster MCNN software systems and tools would be very useful to solve difficult practical image classification problems.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C:\Users\rini\Zotero\storage\4D8BSZ2Z\Zhang - 2018 - Multi-function Convolutional Neural Networks for I.pdf}
}
