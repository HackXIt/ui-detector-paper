%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Klasse Festlegen
%\documentclass[Master,BMR,english]{BASE/twbook}
\documentclass[Bachelor, BIC, english, fhCitStyle, IEEE]{BASE/twbook} % FH definierte Zitierstandards verwenden
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Verwendete Packages
\usepackage[utf8]{inputenc} % Zeichen-Enkodierung (evtl. Abweichungen für Apple)
\usepackage[T1]{fontenc} % Zeichen-Enkodierung
\usepackage{blindtext} % Platzhaltertexte
\usepackage{minted} % Darstellung von Code
\usepackage{comment} % Auskommentieren von ganzen Passagen
\usepackage{csquotes}
\usepackage{algorithm} % Umgebung f Algorithmen
\usepackage[noend]{algpseudocode}
\usepackage{svg}
\usepackage{mdframed}
\graphicspath{ {./PICs/} }
% Wenn Sie während Ihrer Arbeit
% merken, dass Sie zusätzliche Funktionen
% benötigen ist hier ein guter Platz um
% weitere Packages zu laden
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Zitierstil zum selbst definieren
\usepackage[backend=biber, style=ieee]{biblatex} % LaTeX definierter IEEE- Standard
%\usepackage[backend=biber, style=authoryear]{biblatex}      % LaTeX definierter Harvard-Standard
\addbibresource{Literature.bib} % Literatur-File definieren
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Custom commands
% Redefine \section, \subsection, \subsubsection to not add entries to the ToC
% Define a command to temporarily disable \addcontentsline
\newcommand{\nocontentsline}[3]{}
% Define commands to create sections, subsections, and subsubsections without ToC entries
\newcommand{\hidsection}[1]{\bgroup\let\addcontentsline=\nocontentsline\section{#1}\egroup}
\newcommand{\hidsubsection}[1]{\bgroup\let\addcontentsline=\nocontentsline\subsection{#1}\egroup}
\newcommand{\hidsubsubsection}[1]{\bgroup\let\addcontentsline=\nocontentsline\subsubsection{#1}\egroup}
% Defines an alias for writing inline monospace font for code related things
\def\code#1{\texttt{#1}}
% src: https://tex.stackexchange.com/questions/194426/split-itemize-into-multiple-columns
\usepackage{etoolbox,refcount}
\usepackage{multicol}

\newcounter{countitems}
\newcounter{nextitemizecount}
\newcommand{\setupcountitems}{%
  \stepcounter{nextitemizecount}%
  \setcounter{countitems}{0}%
  \preto\item{\stepcounter{countitems}}%
}
\makeatletter
\newcommand{\computecountitems}{%
  \edef\@currentlabel{\number\c@countitems}%
  \label{countitems@\number\numexpr\value{nextitemizecount}-1\relax}%
}
\newcommand{\nextitemizecount}{%
  \getrefnumber{countitems@\number\c@nextitemizecount}%
}
\newcommand{\previtemizecount}{%
  \getrefnumber{countitems@\number\numexpr\value{nextitemizecount}-1\relax}%
}
\makeatother    
\newenvironment{AutoMultiColItemize}{%
\ifnumcomp{\nextitemizecount}{>}{3}{\begin{multicols}{2}}{}%
\setupcountitems\begin{itemize}}%
{\end{itemize}%
\unskip\computecountitems\ifnumcomp{\previtemizecount}{>}{3}{\end{multicols}}{}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Einträge für Deckblatt
\title{Precision at Pixel-Level:\\\acl{yoloAlt} doing UI test automation}

\author{Nikolaus Rieder}
\studentnumber{2010258028}
%\author{Titel Vorname Name, Titel\and{}Titel Vorname Name, Titel}
%\studentnumber{XXXXXXXXXXXXXXX\and{}XXXXXXXXXXXXXXX}

\supervisor{Dr. Dietmar Millinger}
%\supervisor[Begutachter]{Titel Vorname Name, Titel}
%\supervisor[Begutachterin]{Titel Vorname Name, Titel}
%\secondsupervisor{Titel Vorname Name, Titel}
%\secondsupervisor[Begutachter]{Titel Vorname Name, Titel}
%\secondsupervisor[Begutachterinnen]{Titel Vorname Name, Titel}

\place{Wien}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Danksagung/Kurzfassung/Schlagworte
% Abstract
\outline{ Developing and executing test suites on embedded devices with a \ac{gui} can be a time-consuming and exhaustive process. Nevertheless, it is a necessary task in interfaces of critical infrastructure, like \ac{fas} or \ac{hcs} developed at Schrack Seconet. This research analyzes the viability of using machine learning for identifying and detecting \ac{ui} widgets based purely on screenshot data. The analysis is written from the perspective of \ac{ta} performed on critical systems like the aforementioned. The focus is on high performing results due to the qualitative requirements in \ac{ui} test automation of such systems, where upcoming problems during development are considered potentially critical if not detected. \\ During paper development, a \ac{yolo} model was trained on randomly synthesized datasets of \ac{ui} screenshots. The datasets were generated with the \ac{lvgl}, a common and popular library in embedded systems. During development of the datasets and model training processes, key problem factors are identified and potential mitigations addressed. The findings showcase these points in the context of a nurse call system, where reliability of the user interface plays a critical role and margin for error is non-existent. \\ The project follows a Design Science research approach, in which a \ac{ui} generator for \ac{lvgl} was created, that is capable of generating datasets of randomly chosen and placed widgets on a screen, as well as the generation of realistic looking designs by using a system based on a \ac{json} schema. In order to create larger datasets, a \ac{llm} was used to design multiple \ac{ui}s from a list of pre-defined contexts and themes. \\\\ This research aims to reduce time spent in test development and improve test coverage of varied \ac{ui}s in devices of critical systems, while aiding human developers in manual and automated testing. \\ }
\keywords{YOLOv8, UI Testing, embedded devices, widget detection, widget
classification, automated testing}
% Acknowledgements
\acknowledgements{I extend my gratitude to the faculty and staff at UAS Technikum Vienna for their invaluable guidance and the wealth of knowledge they have shared, which has culminated in the completion of this bachelor thesis. Special appreciation is given to my academic advisor, Dietmar Millinger, for his expertise and dedication. Additional thanks are due to Dietmar Millinger, Karl M. Göschka, Lorenz Froihofer, and Susanne Teschl for their inspiration and exceptional contributions to my understanding of machine learning, computer science, and mathematics, respectively. Thanks should also go to my academic peers, for the collaborative environment fostered on our student community server and the valuable networks we have built throughout this educational journey. I am particularly grateful to my partner, whose support and patience were unwavering during my academic endeavors at the UAS Technikum Vienna. I would also like to express my sincere thanks to Schrack Seconet AG for providing me with the opportunity to develop this paper in a professional context. The support from colleagues has been instrumental in aligning my academic pursuits with practical applications in the workplace. Lastly, I thank my own body and mind for not breaking apart throughout the long days and mostly nights that were involved up to this point.}
\setListingsAndAcronyms % Definition der Namen für Quellcodeverzeichnis
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% End of header
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Start of document
\begin{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Title page in FHTW style
\maketitle
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Start of content
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Start of Introduction
\chapter{Introduction}
In embedded systems providing critical infrastructure, it is often mandatory to have well-established and good testing procedures. One such system is nurse call used in hospitals and health care facilities, which consists of devices that allow humans to call for help in situations ranging from mundane \textit{(e.g. requesting patient service)} to life-threatening \textit{(e.g. cardiac arrest alarm)} use-cases. These systems require interfaces for human interaction and such interfaces need to be rigorously tested, to provide feedback on system quality in early stages of development, where the potential for harm is very low. Additionally, automated testing becomes necessary, to reduce possible variation in test execution when compared to manual testing, where human-error is more prevalent than with machines repeating test cases.\\
However, when such interfaces only consist of a touch display, it can become very challenging to automate, as it requires tools for the machine to analyze the visual image and to interact with the \ac{ui} using stimuli \textit{(e.g. simulated touch)}. The automation gets further complicated, the more inputs and outputs the machine needs to deal with in order to perform a representative human test routine. Once such test routines are established, they need to be continously maintained in order to adapt to changes to the \ac{ui}, otherwise such changes can break the test system and halt the overall development process.\\
This research project will demonstrate a machine learning tool to aid in analyzing images to obtain metadata about the displayed \ac{ui}. The metadata consists of position and size of individual widgets, which then can be used in testing routines for precise functional interactions. By training a model that can detect and classify widgets based on screenshots, the testing routines can adapt to many \ac{ui} arrangements dynamically, which can reduce labor involved for maintenance of test repositories.\\
Since covering all possible \ac{ui} libraries and frameworks would be tremendous in scope, the research will limit focus on \ac{lvgl} as the basis for the \ac{ui}. It begins by creating an image and annotation generator for \ac{lvgl}, since there is no available dataset for this library specifically and it also allows for targeted adjustments in dataset size, variation and balance of widget types. Furthermore, it allows for validation based on a real world example from the company where the author is employed at. The work will also be limited to a subset of available widget types, to provide a basic baseline for the viability of the tool, which may be easily extended in the future.\\
Once dataset creation is automated, the paper will demonstrate training based on the \ac{yolo} model and evaluate performance in comparison to human methods. From a testing perspective, this paper is most concerned with the prediction accuracy of the model, since a tool that is not capable of reliable detection will only shift maintenance problems to the tool itself, rather than improving them overall.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Start of Problem description
\chapter{Problem description}
When developing automated test cases for a product which has very long life cycles, due to being incorporated in building infrastructure, of 10 years or more, it can be expected that involved maintenance labor of existing test cases surpasses the creation of new test cases. But even in such long-lasting products, big system changes \textit{(e.g. \ac{ui} redesign, new controller architecture, ...)} eventually occur. Such a change incurs a new evaluation and design of existing test procedures, to accommodate for the new system or \ac{ui}, even if the actual use-case and performed steps fundamentally remain the same.
For the business it is therefor beneficial to have testing tools, which reduce the maintenance costs of such legacy test costs, by being dynamic and adaptable to the given test environment.\\
% THIS PART requires more research in the literature to write a more coherent problem description
% I DON'T EVEN KNOW IF I NEED THIS PART
% Such a change incurs a new evaluation and design of existing test procedures, to accommodate for the new system or \ac{ui}, even if the actual use-case and performed steps fundamentally remain the same. The development of a breaking change must then factor in the costs of changing legacy tests and it may result in decisions that will have less test quality as a result, in favor of bringing in more money for the business.\\
% Therefor it can be beneficial for the tester to make use of tools, which reduce development costs incurred.
% When developing test cases for \aclp{ui}, it is necessary to differentiate between white-box and black-box testing. A white-box testing approach would incorporate knowledge about the code of the \ac{ui}, where a tester would know about the types of widgets used and their applied properties, amongst other things related to the contents of the source. This testing approach is typically used in unit and early integration testing, possibly performed by the developer themselves. In black-box testing, such information is completely stripped away and it is best-practice to leave such work to people outside of the development team to avoid confirmation-bias. % TODO citations for testing approaches, best-practice, etc.
% The context of this paper focuses on the latter with the limitation, that the author has knowledge of the library used and the hardware product itself. The author is a professional in testing and test automation, aware of the risks involved when it comes to knowledge about inner workings of the system under test. % TODO citation of involved risks due to confirmation bias
% As such, the main input for the tool will be limited to what is accessible to the tester, which is constrained to be a black-box testing approach.\\
\section{Problem context at Schrack Seconet}
\clearpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Start of research questions
\chapter{Research questions}
% need to properly define this part
\clearpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Start of literature & theory
\chapter{Existing literature and theoretical basis}

\hidsection{\acf{cnn}}
\hidsubsection{\acf{yolo} architecture}
\begin{figure}
    \caption{Model structure of YOLOv8 detection models by RangeKing}
    \centering
    \includegraphics[width=\textwidth]{yolov8_model_architecture.jpg}
    \label{fig:yolov8-model-architecture}
\end{figure}
\newpage
\hidsection{\acf{lvgl}}
This graphics library is a popular choice amongst embedded system engineers and has seen much usage, due to its good support for a wide range of target architectures and devices. Its performance works well in  resource constrained systems.
\clearpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Start of generator development
\chapter{Development of synthesized datasets}
There generally is no available dataset for the target graphics library (\ac{lvgl}) of this research, so it was mandatory to create one during project development. Creating such a dataset artificially, meaning the images are not based on real world \acp{ui}, has the benefit of being able to adjust the size, variation and balance of the dataset very precisely. It would however be a complicated and time-consuming endeavour, if all of these \aclp{ui} would be created manually using regular \ac{lvgl} code, given that typical datasets for a \ac{ml} model of this type require hundreds of pictures or even more.\\
This is why a generator must be built, which is capable of creating and placing widgets in a structured manner. A screenshot of the displayed window needs to be captured and any metadata (size and position) of placed widgets must be gathered for creating corresponding annotations. The generator will also need to handle variation in visual representation of widgets, otherwise resulting datasets would be biased and detection accuracy of the model would be bad when used on screenshots of real-world examples, which typically incorporate varying visual representations of the same widget type.\\
In order to create variety in individual widgets, a randomized approach is used, where the outputted widgets will have varying styles applied to them. To gain more fine-control on the final visual representation of an created \ac{ui}, an additional design mode is necessary, where the creation of user interfaces can be statically described. The design mode will allow for creation of realistically looking \acp{ui}, while the random mode will allow for the creation of varying visual representation of the individual widgets.\\
Since the creation of a window with corresponding in \ac{lvgl} is typically singular, it will be necessary to have a mechanism to repeatedly call the generator to obtain a new image with each iteration. Using a modular approach, this paper will separate the generator, capable of producing a single image with annotations, from the the so-called randomizer, which will loop the generator repeatedly for an arbitrary amount of iterations. This approach allows for a focused implementation that is capable of producing a good singular image, while having a secondary implementation that solely focuses on the organisation and creation of the dataset. Additionally, when written in the modular approach, it also allows for separation of concerns, where the generator will only need to deal with the intricacies of the \ac{lvgl} API, while the randomizer can focus on the structural requirements of the dataset for the chosen model.\\
\section{First generator version in C}
The initial generator (v1) is forked from the existing VSCode simulator project of \ac{lvgl}. The simulator was stripped from any demo code and re-used as a binary with \ac{cli} arguments. For proper operation, the \code{lv\_conf.h} file was also modified, to increase allocation memory and deactivate unnecessary features. In the configuration, the default color depth was also modified to be 24 bit, so that output images would appear in higher quality. To be able to store screenshots, an additional library named \code{lv\_100ask\_screenshot} was used, which deals with the proper JPEG encoding using the \code{tiny\_jpeg} library.\\
Since the generator uses the allocation and storage mechanisms provided by \ac{lvgl}, it is necessary to use an identifier for the target storage system as prefix to the output filename. For this the \code{\textbackslash} character was chosen, so when supplying the \ac{cli} with a desired output filename, an example argument would look like this: \code{\textbackslash screenshot.jpg}\\
The following usage information will explain the available console arguments for this generator version:
\begin{listing}[htbp]
    \begin{minted}[
        frame=single,
        framesep=2mm,
        baselinestretch=1.2,
        bgcolor=white,
        fontsize=\footnotesize,
        breaklines
        ]{shell-session}
Usage: ./lvgl_ui_generator/build/bin/demo [OPTIONS]
Options:
-m <mode>                   Set the operation mode ("randomizer" or "design")
Randomizer Mode Options:
-w <width>                  Set the width of the UI
-h <height>                 Set the height of the UI
-c <widget_count>           Set the number of widgets to be randomized
-t <widget_types>           Comma-separated list of widget types to be used
-o <output_file>            Set the output file path
-d <delay_count>            Set the screenshot delay count
-l <layout>                 Set the layout type
    \end{minted}
    \caption{Usage instructions of LVGL generator v1}
\end{listing}
\\
\begin{figure}
    \caption{Example output of generator v1 (640x640, 40 random widgets)}
    \centering
    \includegraphics[width=\textwidth]{generator_v1_example.jpg}
    \label{fig:genv1example}
\end{figure}
When run, the generator will produce an image in the provided width and height. Fig. \ref{fig:genv1example}\\It will place randomly chosen widgets up to the given count on the window and use the selected layout (none, grid, flex) to structure them.
\\
The layout structuring of widgets helped in outputting more placement variation in the final image. When using \code{flex}, the created widgets are aligned and will wrap onto the next line if they run out of width. In the \code{grid} layout, widgets would be placed in a grid-like pattern over the whole image. Finally in the \code{none} layout, widgets would be absolutely positioned randomly over the screen, sometimes also causing overlap of widget types.
\clearpage
\subsection{Development issues}
During development and testing of the first generator, it became apparent, that implementing further widget types would create a lot of complexity due to parameter passing in code. Since the C language does not inherently have classes, it was considered to be much more difficult to implement multiple widget types, each of which require a different set of options to be displayed properly.\\
Additionally, issues like segmentation faults due to window deletion occurred, which were inherited from the forked project. The implementation of a design mode was attempted, but this only increased complexity, which resulted in more unreliability of the program, despite the authors experience in the language. Frequent errors in compilation as well as a high compilation time due to the included \ac{lvgl} examples, which the author was not able to remove, were halting and slowing down development time. A lot of features \textit{(widget styles and states, widget property randomization, ...)} were still left to be implemented at a low level and C being a systems programming language, it was becoming too big of a problem for the project.\\\\
The general implementation based on C proved to be much more error-prone as a result of these issues and was therefor considered too complex for usage in a generation pipeline. A new solution was sought after, which would allow implementation of the generator in a more resilient and forgiving programming language.\\
\ac{lvgl} provides C bindings for usage and implementation in other programming languages. A language promoted by the creators themselves is micropython, which is a reduced implementation of python for running on hardware. While the issues faced with the initial generator certainly are solvable, there existed no premise to continue in the C language. A new version in that language was considered to be a simpler approach that would refocus the project on the dataset creation matter at hand.
\clearpage
\section{Second generator version in Micropython}
The second generator version is implemented in micropython, and was started as a fresh project using the \code{lv\_micropython} project as a necessary sub-module for compiling the micropython binary including the \code{lv\_bindings} module.\\
Rewriting the functionality of the initial generator was much simpler, as the binary comes with \code{argparse} for easily creating a \ac{cli} interface. Having the functionality of classes, it was also easier to differentiate between the two modes with their own source files. Widget creation was divided into separate functions, one for each widget type, and then called via a mapping of the type name to the creation function. This proved to be much more maintainable throughout project development and issues were able to be faced on an individual basis per implemented type.\\
Both modes provide a function to retrieve the top-level container and all its children in a dictionary object format, containing all relevant metadata information gathered through usage of the \code{coords\_t} object. This \ac{lvgl} object is necessary, as it stores the coordinates of a widget in relation to the whole window, while other methods usually provide this information in relation to the parent container that the widget resides in.\\
The retrieved container is passed to a screenshot function, which uses a manually added JPEG encoding from the tinyjpeg project to create an image. Using the coordinate metadata, the annotation file of the resulting image is created in \ac{yolo} format (\code{class center\_x center\_y center\_width center\_height}) and also normalized if the user provides the corresponding flag.
A resulting example image using design mode is illustrated in Fig.\ref{fig:genv2example} and its corresponding annotation file is listed below.\\
The following widget types were implemented and used in this generator version:
\begin{itemize}
  \begin{AutoMultiColItemize}
    \item Arc
    \item Bar
    \item Button
    \item Buttonmatrix
    \item Calendar
    \item Checkbox
    \item Dropdown
    \item Label
    \item Roller
    \item Scale
    \item Slider
    \item Spinbox
    \item Switch
    \item Table
    \item Textarea
  \end{AutoMultiColItemize}
\end{itemize}
% TODO arrange the figure and annotation to be alongside each other
\begin{figure}
    \caption{Example output of generator v2 (640x640, widget\_showcase.json)}
    \centering
    \includegraphics[height=0.4\textheight]{generator_v2_example.jpg}
    \label{fig:genv2example}
\end{figure}
\begin{listing}[htbp]
    \begin{minted}[
        frame=single,
        framesep=2mm,
        baselinestretch=1.2,
        bgcolor=white,
        fontsize=\footnotesize
        ]{shell-session}
textarea 0.3625 0.784375 0.234375 0.15625
table 0.6625 0.821875 0.3328125 0.2296875
checkbox 0.2046875 0.140625 0.04375 0.03125
arc 0.5609375 0.2109375 0.203125 0.203125
label 0.0734375 0.2875 0.0390625 0.025
label 0.0859375 0.328125 0.065625 0.025
label 0.478125 0.065625 0.0375 0.025
label 0.7078125 0.065625 0.059375 0.025
label 0.1109375 0.1375 0.1140625 0.025
bar 0.2609375 0.2859375 0.15625 0.0203125
label 0.0921875 0.184375 0.078125 0.025
label 0.1421875 0.246875 0.1765625 0.025
switch 0.2234375 0.1953125 0.08125 0.046875
buttonmatrix 0.2296875 0.4875 0.40625 0.203125
button 0.1296875 0.08125 0.1515625 0.05625
calendar 0.6015625 0.5390625 0.3046875 0.3046875
scale 0.7796875 0.2109375 0.203125 0.203125
slider 0.2609375 0.3265625 0.15625 0.0203125
roller 0.8328125 0.4890625 0.128125 0.20625
dropdown 0.128125 0.7375 0.203125 0.0625
    \end{minted}
    \caption{Generated annotation file for example output of generator v2}
\end{listing}
\clearpage
\hidsubsubsection{Distorted output images due to render race conditions}
The used snapshot API sometimes causes output issues, since race-conditions can occur due to render updates while the referenced image data is still being encoded, resulting in distorted or sheared images as seen in the example Fig.\ref{fig:genv2distortedexample}. This largely originates from adding the JPEG encoding in micropython, which inherently is much slower than its C counterpart. While the issue was not solved during paper development due to its late discovery, a problem solution using a compiled C module of the latest tinyjpeg version was discussed in development forums of \ac{lvgl}. Adding this module will improve encoding speed and thus not face race conditions due to underlying render updates by \ac{lvgl}. Images which had this issue needed to be regenerated and datasets were fixed manually.
\begin{figure}
    \caption{Example image with distortions from generator v2}
    \centering
    \includegraphics[height=0.45\textheight]{distorted_image_example.jpg}
    \label{fig:genv2distortedexample}
\end{figure}
\hidsubsubsection{Memory allocation errors}
Since the image encoding takes place in micropython, it actually takes up a lot more memory in heap. When creating and encoding images with the generator, it may result in memory allocation errors, depending on available memory on the running machine. This issue is not solved in the final generator version of the paper and was circumvented by retry mechanisms when creating image files using the dataset pipeline.
\newpage
\section{Random mode}
In random mode, the generator will produce a window of desired width and height. It randomly chooses widgets from the user provided type list and places them into a container. Since absolute positioning proved to be most suitable for random generation, the implementation of layouts was omitted. To avoid overlap, a spatial map was used, to find free space in the window during widget creation and placement.\\
The usage description in Listing \ref{code:genv2usage} provides an overview of the parameters for this mode.
\begin{listing}[htbp]
    \begin{minted}[
        frame=single,
        framesep=2mm,
        baselinestretch=1.2,
        bgcolor=white,
        fontsize=\footnotesize,
        breaklines
        ]{shell-session}
usage: src/main.py [-h] [-m, --mode mode] [-?, --usage] [-n, --normalize] [-o, --output_file output_file] [-W, --width width] [-H, --height height] [-c, --widget_count widget_count] [-t, --widget_types widget_types+] [-l, --layout layout] [--random-state]

Process CLI arguments for the UI generator.

optional args:
  -h, --help                        show this message and exit
  -m, --mode                        mode the mode to run the program in
  -?, --usage                       Print usage information for that mode.
  -n, --normalize                   normalize the bounding boxes
  -o, --output_file output_file     The output file (screenshot)
  -W, --width width                 the width of the UI
  -H, --height height               the height of the UI
  -c, --widget_count widget_count   the count of widgets
  -t, --widget_types widget_types+  A list of widget types
  -l, --layout layout               the layout option
  --random-state                    Use a random state for each created widget (experimental)
    \end{minted}
    \caption{Usage instructions of LVGL generator v2 in random mode}
    \label{code:genv2usage}
\end{listing}\\\\
\subsection{Style and state variation}
When using random mode, the generator will randomize applied style of widgets to artificially create variation in created datasets. Additionally, there is an experimental feature, which will also randomize the state of widgets. The latter is not recommended for usage in training, as sometimes this might cause widgets to not be displayed if their state is invalid or disabled, which will confuse the model, as the created metadata will still include the annotation of said widget.\\
Listing \ref{code:randomized-properties} shows the properties and their range of possible values to be randomized. Listing \ref{code:randomized-state} shows the available states.
\begin{listing}[htbp]
    \begin{minted}[
        frame=single,
        framesep=2mm,
        baselinestretch=1.2,
        bgcolor=white,
        fontsize=\footnotesize,
        breaklines
        ]{python}
# List of style properties to randomize
properties = [
    ('set_bg_color', lv.color_make),
    ('set_bg_opa', lambda: random.randint(0, 100)),
    ('set_border_color', lv.color_make),
    ('set_border_opa', lambda: random.randint(0, 100)),
    ('set_border_width', lambda: random.randint(0, 10)),
    ('set_outline_width', lambda: random.randint(0, 10)),
    ('set_outline_color', lv.color_make),
    ('set_outline_opa', lambda: random.randint(0, 100)),
    ('set_shadow_width', lambda: random.randint(0, 15)),
    ('set_shadow_offset_x', lambda: random.randint(0, 10)),
    ('set_shadow_offset_y', lambda: random.randint(0, 10)),
    ('set_shadow_color', lv.color_make),
    ('set_shadow_opa', lambda: random.randint(0, 100)),
    ('set_line_width', lambda: random.randint(0, 10)),
    ('set_line_dash_width', lambda: random.randint(0, 10)),
    ('set_line_dash_gap', lambda: random.randint(0, 10)),
    ('set_line_rounded', lambda: random.choice([True, False])),
    ('set_line_color', lv.color_make),
    ('set_line_opa', lambda: random.randint(0, 100)),
    ('set_text_color', lv.color_make),
    ('set_text_opa', lambda: random.randint(0, 100)),
    ('set_text_letter_space', lambda: random.randint(0, 10)),
    ('set_text_line_space', lambda: random.randint(0, 10)),
    ('set_opa', lambda: random.randint(0, 100)),
    ('set_align', lambda: random.choice([lv.ALIGN.CENTER, lv.ALIGN.TOP_LEFT, lv.ALIGN.TOP_RIGHT, lv.ALIGN.TOP_MID, lv.ALIGN.BOTTOM_LEFT, lv.ALIGN.BOTTOM_RIGHT, lv.ALIGN.BOTTOM_MID, lv.ALIGN.LEFT_MID, lv.ALIGN.RIGHT_MID, lv.ALIGN.DEFAULT])),
    ('set_pad_all', lambda: random.randint(0, 10)),
    ('set_pad_hor', lambda: random.randint(0, 10)),
    ('set_pad_ver', lambda: random.randint(0, 10)),
    ('set_pad_gap', lambda: random.randint(0, 10)),
    ('set_pad_top', lambda: random.randint(0, 10)),
    ('set_pad_bottom', lambda: random.randint(0, 10)),
    ('set_pad_left', lambda: random.randint(0, 10)),
    ('set_pad_right', lambda: random.randint(0, 10)),
    ('set_pad_row', lambda: random.randint(0, 10)),
    ('set_pad_column', lambda: random.randint(0, 10)),
    ('set_margin_top', lambda: random.randint(0, 10)),
    ('set_margin_bottom', lambda: random.randint(0, 10)),
    ('set_margin_left', lambda: random.randint(0, 10)),
    ('set_margin_right', lambda: random.randint(0, 10))
]
    \end{minted}
    \caption{Available style properties which are randomized in given value range}
    \label{code:randomized-properties}
\end{listing}
\begin{listing}[htbp]
    \begin{minted}[
        frame=single,
        framesep=2mm,
        baselinestretch=1.2,
        bgcolor=white,
        fontsize=\footnotesize,
        breaklines
        ]{python}
def randomize_state(widget: lv.obj):
    if hasattr(widget, "set_state"):
        state = random.choice([lv.STATE.CHECKED, lv.STATE.DISABLED, lv.STATE.FOCUSED, lv.STATE.PRESSED, lv.STATE.HOVERED, lv.STATE.EDITED])
        widget.set_state(state, True) # Add the state
    else:
        raise AttributeError(f"Widget {widget} does not have a 'state' property.")
    \end{minted}
    \caption{Available state properties which are randomized in given options}
    \label{code:randomized-state}
\end{listing}
\clearpage
\section{Design mode}
In design mode, the generator will parse a provided \ac{json} file and create a \ac{ui} according to the specified design. This mode allows for creation of a static \ac{ui} design, which more resembles the expectation of a realistic interface in comparison to the random mode. To allow for variation in this mode, a special type \code{random} is available, which randomly chooses widgets from a given type list and randomizes their appearance. Having this option, allows for randomization in specific portions of the \ac{ui}, while keeping the rest statically defined. In the design file, various styles can be defined. Multiple styles can then be applied to individual widgets and containers by name reference. In general, all possible \textit{setter} properties of \ac{lvgl} styles are allowed, as the implementation checks for the availability of the corresponding setter attribute on a \code{lv.style\_t} object.\\
Since the design mode only requires a single parameter \textit{(i.e. the path to the design file)}, no further usage instructions are provided in this paper.
\hidsubsection{Design file rules}
The creation of design files require a certain set of rules that need to be followed, in order for the creation to work properly.
\begin{enumerate}
    \item It is mandatory that the first widget object in \code{root} is a container, as the root widget is always a container.
    \item The title of the window is not mandatory and not used by the generator. It is only there for reference to the user for describing and recognizing the contents of a design file.
    \item The \code{styles} object is optional and can be omitted if no styles are defined.
    \item Added styles are referenced by their name in the \code{style} array of each widget. If a style is not found, the generator will throw a \code{ValueError}.
    \item A style defines a list of properties that are applied to widgets via the usage of a \code{lv.style\_t} object. The possible properties are the same as documented in the \ac{lvgl} API for styles.
% (https://docs.lvgl.io/9.1/API/misc/lv\_style\_gen.html#)
Properties are verified by checking if the specified name has a corresponding \code{setter} attribute in the object. This is done by appending \code{set\_} to the property name, thus the user is required to use the property setter function names without the prefix. For example, to set the background color of a widget, one would use the property \code{bg\_color}. The generator will then look for the \code{set\_bg\_color} attribute in the object and apply the converted value to it.
    \item If a provided property inside a style object does not actually correspond to an available attribute, the generator will ignore it and continue.
    \item Values supplied to style properties are converted according to the required type of the property. Some properties taking in special objects, like colors, require a specific string to be supplied (e.g. \code{\#RRGGBB} for any color property or \code{top-left} for the align property).
    \item If value conversion fails, the property is ignored and the generator will ignore it and continue.
    \item The id property is mandatory for widgets of type container, as it is required to reference the container inside the children array, when the special widget type random is used.
    \item The special widget type random may be used to supply a list of widget types for the generator to randomly choose from and then create a random widget in similar fashion to the random mode. This is useful for randomizing widgets in certain areas of the UI, while keeping the rest of the UI static.
\end{enumerate}
\newpage
\subsection{JSON schema}
In order to validate design files before providing them to the generator, a regular python script is used. This was not included in the generator, as the corresponding package is not available in micropython. The schema was purposely made more strict than what the generator requires, so that validation errors could be provided to the later used \ac{llm}, to offer more detailed guidance during design generation.
Listing \ref{code:validate-design-script} shows a simple example script to validate a design file with the available schema in the source repository.
% Link to schema: https://raw.githubusercontent.com/HackXIt/ui-detector/74795557012d5750fa35f1ff5a774d3172c6e953/schema/design_file.schema.json
\begin{listing}[htbp]
    \begin{minted}[
        frame=single,
        framesep=2mm,
        baselinestretch=1.2,
        bgcolor=white,
        fontsize=\footnotesize,
        breaklines
        ]{python}
def load_json_file(filepath: str):
    import json
    with open(filepath, 'r') as f:
        return json.load(f)

def verify_design_from_file(design_file: str, schema_file: str) -> tuple[bool, Exception]:
    from jsonschema import validate
    from jsonschema.exceptions import ValidationError
    design = load_json_file(design_file)
    schema = load_json_file(schema_file)
    try:
        validate(instance=design, schema=schema)
        print(f"Provided design file {design_file} is valid.")
        return True, None
    except ValidationError as e:
        print(f"Provided design file {design_file} is invalid:\n{e}")
        return False, e

if __name__ == '__main__':
    verify_design_from_file('path/to/design_file.json', 'path/to/design_file.schema.json')
    \end{minted}
    \caption{Exemplary validation code for design file using JSON schema}
    \label{code:validate-design-script}
\end{listing}
\clearpage
\section{Dataset creation}
Both generator modes only ever produce a single image output with a corresponding text file in the \ac{yolo} annotation format. This choice was intended to allow for a separate implementation which will handle the organization of created files, by repeatedly calling the generator and moving the outputs into a dataset folder structure. It was also a necessary choice, as the micropython programming language would have been very limiting for implementing third-party integration.\\
Furthermore, it also allowed for balancing the representation of classes in random mode, by removing a type from the provided widget list, once a certain threshold of classes is reached.\\
The dataset creation concept is further illustrated in Fig.\ref{fig:dataset-creation-random} and Fig.\ref{fig:dataset-creation-design}.\\
The produced annotations in each iteration are post-processed before they are moved into the dataset structure. Widgets that are placed out-of-bounds of the window have annotation values which are not usable for model training, as such they will be removed from the annotation file during this step. The label modifications are generally minor, since they occur rarely, and their affect on model performance is negligible. In the annotations, the written class names also need to be processed, since the \ac{yolo} model cannot interpret classes from their string representation, so they are replaced by their index in the list of implemented types.
\begin{figure}[!htbp]
    \caption{Dataset creation concept using random mode}
    \centering
    \includesvg[width=\textwidth,keepaspectratio]{PICs/Framework-Generic_generator-random}
    \label{fig:dataset-creation-random}
\end{figure}
\begin{figure}[!htbp]
    \caption{Dataset creation concept using design mode with gpt}
    \centering
    \includesvg[width=\textwidth,keepaspectratio]{PICs/Framework-Generic_design-mode-gpt}
    \label{fig:dataset-creation-design}
\end{figure}
\clearpage
\subsection{Design contexts and themes}
Since creation of a large design dataset would additionally require the labor-intensive manual definition of a large set of designs, it was decided to use a \ac{llm} which would handle the creation of those design files. For that purpose, a list of 100x contexts and 100x themes was created. By unique pair combination, this creates a possible list of 10000 design contexts and themes to be provided to the \ac{llm}.\\
As illustrated in Fig.\ref{fig:dataset-creation-design}, the \ac{llm} would first be instructed to write out a design idea in the provided \ac{ui} context and style theme. Via a system prompt, the model is instructed to include the widgets to be used in the design, a high-level description of the \ac{ui} purpose and a structural description of what the \ac{ui} shall look like.\\
The idea output is then again fed into the \ac{llm} in a separate context and instructed to output a valid \ac{json} file. It additionally receives the \ac{json} schema as guidance on how to structure and construct the output \ac{json}. The resulting output of the model is then validated against the schema and any validation errors are fed back to the \ac{llm} for correction. Once an output design is valid, it will be stored and added to a design file list, to be generated in a following step. This loop can be repeated for a desired amount of design files, for up to 10000 generations, given our combination list of contexts and themes.
\subsection{Third-party integration} % ClearML & LLM (ChatGPT)
The dataset creation process relies heavily on third-party integration. To track individual versions of datasets, a popular experiment tracker named ClearML was used. In order to automate the design file creation process, the proprietary \ac{llm} named ChatGPT was used via OpenAIs available python package and API.
\hidsubsubsection{Integration with ClearML}
Whenever a dataset creation starts, a new ClearML task is created and used for generating statistics about the process. The task stores all of the following information:
\begin{itemize}
    \item Console output of the script
    \item Scalar plots displaying generation statistics over iterations
    \item Metadata about dataset (file split of train,val,train)
    \item Artifacts (ZIP folder of dataset contents)
    \item Parameters of the script/task
    \item Machine environment information
    \item Version control information (diff, commit hash, ...)
\end{itemize}
All of that surrounding information regarding the creation process helps in getting a better understanding about errors and quality of the created dataset.
\hidsubsubsection{Integration with LLM (gpt-4-turbo)}
In order to generate design files at a larger scope, OpenAIs gpt-4-turbo model was used in the dataset creation process when using the design mode of generator v2. In order for the model to properly behave and generate usable output, the \ac{gpt} was instructed via system prompts.
The prompt was designed, so that the format of the idea is structurally similar through each iteration and proper output from the \ac{json} generation step can be expected. The used system prompt for the design \ac{json} can be seen in Listing \ref{code:gpt4-design-json-system-prompt}. Due to their length, the system prompt for the design idea and corresponding examples are omitted from the paper, but can be viewed in the source code repository.
\begin{listing}[htbp]
    \begin{minted}[
        frame=single,
        framesep=2mm,
        baselinestretch=1.2,
        bgcolor=white,
        fontsize=\footnotesize,
        breaklines
        ]{text}
You are a UI generator.
Your goal is to create a new single window UI using a specialized JSON format.
The format specification is available in the design.schema.json file below.
Follow the provided design guideline of the user when replicating the design idea using the structure of the JSON format.
Always output a valid JSON object that represents the UI design.

You ALSO MUST adhere to the following SPECIAL RULES:
- Never use 'grid' container. Use either 'none' or 'flex' container.
- Containers must be used to set style for the whole window or the specific group of widgets (such as background color and other general styles).
- Each widget placed inside a 'none' container must have an associated style defining its X and Y coordinates.
- Widgets may have multiple styles applied to them
- Window size MUST be 640 x 640 pixels
- You MUST make sure that coordinates of widgets do not overlap due to width and height of the widgets
- You MUST make sure that the widgets are within the window bounds (0, 0, 640, 640)
    \end{minted}
    \caption{System prompt for gpt-4-turbo generating design file (\ac{json})}
    \label{code:gpt4-design-json-system-prompt}
\end{listing}
\clearpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Start of model training
\chapter{Training of YOLO model}
The training process illustrated in Fig.\ref{fig:yolo-train} uses the already built training mechanisms of ultralytics \ac{yolo} model version 8. All experiments of the paper were performed using this model version and the python package version used was 8.1.47.
\begin{figure}[!htbp]
    \caption{YOLOv8 model training process}
    \centering
    \includesvg[width=\textwidth,keepaspectratio]{PICs/Framework-Generic_model-train}
    \label{fig:yolo-train}
\end{figure}
\section{Training tasks in ClearML}
Every training process of a \ac{yolo} model is automatically tracked via the existing integration of ClearML in the ultralytics \ac{yolo} engine. This was a crucial step in the creation of reproducible results, since all hyper-parameters of the training process are stored and the used dataset is referenced by a unique ID. This also allows for later modification of a training process, by cloning of the training task and manual editing of parameters in the cloned task.
\subsection{Training agents on local and remote machines}
In order to train models remotely on a more powerful machine than the authors laptop, a ClearML agent was setup using Google Colab. The creators of ClearML provide a simple mechanism and already defined notebook for starting such an agent. By having this agent readily available, any local or previously started training tasks could be remotely executed. The built integration uses docker containers to execute individual tasks and execution was therefor immutable once finalized. Such an agent was also started on the local machine of the author, to compare results between the two machines and for debugging purposes of the pipeline itself.
\section{Tuning for best hyperparameters}
In order to optimize the training process for a given dataset to determine the best set of hyper-parameters, the existing tuning process from the ultralytics engine is used. This tuning process has automatic optimization features, choosing different optimizer algorithms for different epochs.
\section{Hyperparameter optimization}
The tune process by ultralytics was not always best for producing results. In some cases, the process failed to record the output model weights, despite reporting good results, which led to the creation of a custom \ac{hpo} pipeline.\\
ClearML offers mechanisms for performing \ac{hpo} on previously executed training tasks. For the optimization of parameters, the Optuna optimizer is used, which offers an algorithm to optimize for a given metric using either discrete value ranges or a defined set of scalar values. The open-source optimizer implementation would then determine the best hyper-parameters for the training task by running it multiple times with a different set of parameters determined by the algorithm, while tracking the target performance metrics. In all \ac{hpo} processes performed, the configured target metric was the \ac{map}, which shows the model performance across different levels of detection difficulty. The \ac{yolo} model provides two \ac{map} metrics:
\begin{itemize}
    \item \ac{map}50 - Mean Average Precision at 50\% \ac{iou} threshold
    \item \ac{map}50-95 - Mean Average Precision averaged over \ac{iou} thresholds from 50\% to 95\% (at 5\% steps)
\end{itemize}
The author chose the \ac{map}50-95 metric, as it provided a more comprehensive view of the models performance, which was representative of an overall better accuracy and recall.
\clearpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Start of results
\chapter{Research results}
The final project resulted in the creation of custom dataset generator capable of producing \ac{ui} screenshot images with corresponding widget annotations. It provides two modes of operation, which can be used to create datasets of varying image sizes, styling properties and used widget types.
In addition to that, a \ac{json} design parser was written, which converts a design specification written in accordance to a \ac{json} schema into an image representing a realistically looking \ac{ui}.\\
Multiple pipelines with integration to ClearML were written, to aid in dataset generation, \ac{yolo} model training and \acf{hpo}.
\section{Model performance}
The created generators were used in the research to train a baseline model based on a random dataset. The performance of this baseline model showed such good results, that the author doubted its viability in real-world scenarios, since the resulting metric scores showcased a form of over-optimization for the case of detecting widget types on a white background.\\
In response to that, another dataset using the implemented design mode was created with the help of an \ac{llm} to produce the necessary design files. The main purpose of the design mode is to create more realistic looking \acp{ui}, with an overall theme applied in the form of styles, which got rid of the white background and allowed the widgets to blend in more within the window.\\
The improved model was trained on the generated design dataset and shows a decrease in performance when compared to the baseline model. The drop in performance is attributed to the increased difficulty for the model when detecting widgets on colored and styled backgrounds, which slightly blends the visible features of individual widgets with their surroundings, making them harder to detect.\\
Nevertheless, the model showed very good performance for a few widget types, which demonstrates the viability of using machine learning for detecting widget types based on screenshot data, even though the set widget types is limited.
\subsection{Random based dataset}
The random dataset contains 15 different widget types of \ac{lvgl} in varying positions and widget styles (excluding container style), where each widget is represented about 100 times. The images of that dataset each contained 12 widget instances, resulting in 128 files (including a little bit of overshoot in widget type representation). The dataset is uploaded to the repository of the paper and can be referenced by ID \code{4995cded1302441ab74d514d4487952a}.
\subsection{Design based dataset}
The design dataset has 145 design files, resulting in a total of 1173 widgets (including containers) spread over 137 generated image files. While the balancing of the design datasets has room for improvements, with certain types being overly preferred by the \ac{llm}, it did affect model performance greatly. The uploaded dataset can be referenced by ID \code{5c8e248bdcc24ee2bb57760f0961c690}. Due to some errors produced by the generation process, manual modifications were necessary, which resulted in the corrected dataset ID \code{8b8abeecffc340e3a839b982d7d49ef0}.
\section{Impact of style variation}
An important factor discovered during the research is the importance of style variation when training a \ac{yolo} model on \ac{ui} image data. Given that in the real-world \acp{ui} need to appeal to humans and also be usable by them, it is important to incorporate style variations into training datasets. This avoids over-optimized datasets illustrating best-case detection scenarios where individual widget features would be more easily detected by the model. Incorporating themes and contexts into artificially generated \ac{ui} brings the benefit of creating datasets, that are more aligned with worst-case detection scenarios, where individual widgets need to be differentiated with better precision. It is recommended to experiment with outlines and other style properties when creating such datasets and training future models in the context of \ac{ui} widget detection.
\clearpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Start of discussion
\chapter{Discussion}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% End of content
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Start of listings
\clearpage % Beginne neue Seite

\printbib % Literaturverzeichnis LaTeX-Zitier-Standard
%\printbib{Literatur}                                             % Literaturverzeichnis FH-Zitier-Standard
\clearpage

\listoffigures % Abbildungsverzeichnis
\clearpage

\listoftables % Tabellenverzeichnis
\clearpage

\listoflistings % Quellcodeverzeichnis
\clearpage

\phantomsection
\addcontentsline{toc}{chapter}{\listacroname}
\chapter*{\listacroname}
% TODO REMOVE UNUSED ACRONYMS AT THE END OF WRITING
\begin{acronym}
    \acro{yoloAlt}[YOLO]{You only live once}
    \acro{yolo}[YOLO]{You Only Look Once}
    \acro{yolo9}[YOLOv9]{YOLO version 9 by ultralytics}
    \acro{yolo8}[YOLOv8]{YOLO version 8 by ultralytics}
    \acro{yolo3}[YOLOv3]{YOLO version 3 by Joseph Redmon}
    \acro{ui}[UI]{user interface} \acro{gui}[GUI]{graphical user interface}
    \acro{ta}[TA]{test automation} \acro{sut}[SUT]{system under test}
    \acro{dut}[DUT]{device under test} \acro{hil}[HiL]{Hardware-in-the-Loop}
    \acro{sil}[SiL]{Software-in-the-Loop} \acro{pil}[PiL]{Processor-in-the-Loop}
    \acro{mil}[MiL]{Model-in-the-Loop}
    \acro{json}[JSON]{JavaScript Object Notation}
    \acro{cli}[CLI]{Command-line interface}
    \acro{mcu}[MCU]{Microcontroller unit}
    \acro{rtos}[RTOS]{Real-Time Operating System}
    \acro{led}[LED]{Light Emitting Diode}
    \acro{lcd}[LCD]{Liquid Crystal Display}
    \acro{pwm}[PWM]{Pulse Width Modulator}
    \acro{iso}[ISO]{International Standards Organization}
    \acro{lvgl}[LVGL]{Light and versatile graphics library}
    \acro{Lvgl}[LittlevGL]{Light and versatile graphics library}
    \acro{fas}[FAS]{fire alarm system}
    \acro{hcs}[HCS]{health care system}
    \acro{ml}[ML]{machine learning}
    \acro{mlops}[MLOPS]{machine learning operations}
    \acro{hpo}[HPO]{hyper-parameter optimization}
    \acro{iou}[IoU]{Intersection over Union}
    \acro{map}[mAP]{mean average precision}
    \acro{llm}[LLM]{large language model}
    \acro{gpt}[GPT]{generative pre-trained transformer}
    \acro{cnn}[CNN]{Convolutional Neural Network}
\end{acronym}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% End of listings
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Start of appendix
\clearpage
\appendix
\chapter{... appendix ...}
\clearpage
\end{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% End of appendix